{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae360a05-9308-4a72-ae62-1c32dfadb3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers import AutoTokenizer, RobertaForMaskedLM, RobertaForCausalLM\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "# No token_type_ids, just separate sequences with tokenizer.sep_token\n",
    "# So I guess RobertaTokenizer automatically adds <s> and </s> tokens to input\n",
    "# cls token is aparently <s> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8b01826-a21c-4ba1-972b-a0bd56bbac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Same as BertEmbeddings with a tiny tweak for positional embeddings indexing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Copied from transformers.models.bert.modeling_bert.BertEmbeddings.__init__\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        # self.register_buffer(\n",
    "        #     \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n",
    "        # )\n",
    "    \n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.position_embeddings = nn.Embedding(\n",
    "            config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "\n",
    "        def create_position_ids_from_input_ids(input_ids, padding_idx):\n",
    "        \n",
    "            # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n",
    "            mask = input_ids.ne(padding_idx).int()\n",
    "            incremental_indices = torch.cumsum(mask, dim=1).type_as(mask) * mask\n",
    "            return incremental_indices.long() + padding_idx\n",
    "    \n",
    "\n",
    "        position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx)\n",
    "            \n",
    "        embeddings = self.word_embeddings(input_ids)\n",
    "                \n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        \n",
    "        embeddings += position_embeddings\n",
    "        \n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "class RobertaSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward( self, hidden_states, attention_mask):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "    \n",
    "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask is (precomputed for all layers in RobertaModel forward() function)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(new_context_layer_shape)\n",
    "\n",
    "        outputs =  context_layer\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Copied from transformers.models.bert.modeling_bert.BertSdpaSelfAttention with Bert->Roberta\n",
    "class RobertaSdpaSelfAttention(RobertaSelfAttention):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def forward( self, hidden_states, attention_mask = None):\n",
    "        \n",
    "        bsz, tgt_len, _ = hidden_states.size()\n",
    "\n",
    "        query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
    "        \n",
    "        current_states = hidden_states\n",
    "        attention_mask = attention_mask\n",
    "\n",
    "        key_layer = self.transpose_for_scores(self.key(current_states))\n",
    "        value_layer = self.transpose_for_scores(self.value(current_states))\n",
    "\n",
    "        attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "            query_layer,\n",
    "            key_layer,\n",
    "            value_layer,\n",
    "            attn_mask=attention_mask,\n",
    "            dropout_p=0.0,\n",
    "            is_causal=False,\n",
    "        )\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "        attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n",
    "\n",
    "        outputs = attn_output\n",
    "\n",
    "        return outputs\n",
    "\n",
    "class RobertaSelfOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "class RobertaAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self = RobertaSdpaSelfAttention(config)\n",
    "        self.output = RobertaSelfOutput(config)\n",
    "\n",
    "    def forward(self, hidden_states,attention_mask = None):\n",
    "        \n",
    "        self_outputs = self.self( hidden_states, attention_mask)\n",
    "        attention_output = self.output(self_outputs, hidden_states)\n",
    "        \n",
    "        return attention_output\n",
    "\n",
    "class RobertaIntermediate(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.intermediate_act_fn = nn.GELU()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class RobertaOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "        \n",
    "class RobertaLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = RobertaAttention(config)\n",
    "        self.intermediate = RobertaIntermediate(config)\n",
    "        self.output = RobertaOutput(config)\n",
    "\n",
    "    def forward( self, hidden_states, attention_mask = None):\n",
    "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
    "        self_attention_outputs = self.attention( hidden_states, attention_mask)\n",
    "        attention_output = self_attention_outputs\n",
    "\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        \n",
    "        return layer_output\n",
    "\n",
    "\n",
    "class RobertaEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([RobertaLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward( self, hidden_states, attention_mask = None):\n",
    "        \n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "          \n",
    "            layer_outputs = layer_module(\n",
    "                hidden_states,\n",
    "                attention_mask\n",
    "            )\n",
    "\n",
    "            hidden_states = layer_outputs\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class RobertaModel(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = RobertaEmbeddings(config)\n",
    "        self.encoder = RobertaEncoder(config)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask = None):\n",
    "   \n",
    "        input_shape = input_ids.size()\n",
    "        batch_size, seq_length = input_shape\n",
    "        \n",
    "        device = input_ids.device\n",
    "    \n",
    "        embedding_output = self.embeddings(input_ids=input_ids)\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones((batch_size, seq_length), device=device)\n",
    "\n",
    "        use_sdpa_attention_masks = True\n",
    "\n",
    "        extended_attention_mask = attention_mask\n",
    "        # # Expand the attention mask\n",
    "        # if use_sdpa_attention_masks and attention_mask.dim() == 2:\n",
    "        #     # Expand the attention mask for SDPA.\n",
    "        #     # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n",
    "        #     extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n",
    "        #         attention_mask, embedding_output.dtype, tgt_len=seq_length\n",
    "        #     )\n",
    "        # else:\n",
    "        #     # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        #     # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        #     extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n",
    "\n",
    "        encoder_outputs = self.encoder( embedding_output, attention_mask=extended_attention_mask)\n",
    "        \n",
    "        return encoder_outputs\n",
    "\n",
    "class RobertaLMHead(nn.Module):\n",
    "    \"\"\"Roberta Head for masked language modeling.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        \n",
    "        self.gelu = nn.GELU()\n",
    "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = self.dense(features)\n",
    "        x = self.gelu(x)\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        # project back to size of vocabulary with bias\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class RobertaClassificationHead(nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features):\n",
    "        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "191acc96-5e72-44ea-b986-3ebbeef9fb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaMaskedLM(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.lm_head = RobertaLMHead(config)\n",
    "        self.config = config\n",
    "\n",
    "        # weight tying between input embedding and prediction head \"de-embedding\"\n",
    "        self.lm_head.decoder.weight = self.roberta.embeddings.word_embeddings.weight \n",
    "\n",
    "\n",
    "    def forward( self, input_ids, attention_mask = None, labels = None):\n",
    "\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        sequence_output = outputs\n",
    "        prediction_scores = self.lm_head(sequence_output)\n",
    "\n",
    "        masked_lm_loss = None\n",
    "        if labels is not None:\n",
    "            # move labels to correct device to enable model parallelism\n",
    "            labels = labels.to(prediction_scores.device)\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "\n",
    "       \n",
    "        output = prediction_scores\n",
    "        return output\n",
    "        # return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type = \"FacebookAI/roberta-base\"):\n",
    "        \"\"\" Loading pretrained Roberta weights from hugging face \"\"\"\n",
    "        # print(\"loading weights for %s\" % model_type)\n",
    "\n",
    "        # Random init of model\n",
    "        config = RobertaConfig()\n",
    "        model = RobertaMaskedLM(config)\n",
    "        \n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "\n",
    "        # Init a Roberta from hugging face \n",
    "        model_hf = RobertaForMaskedLM.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "        sd_hf = model_hf.state_dict()\n",
    "        sd_hf_keys = [k for k in sd_hf.keys() if not k.endswith('lm_head.bias')]\n",
    "        # Copy over weights. State Dicts are currently in same order, so I can just blind copy \n",
    "        for keys in zip(sd_keys, sd_hf_keys):\n",
    "            # print(sd[keys[0]].shape)\n",
    "            # print(sd_hf[keys[1]].shape)\n",
    "            \n",
    "            assert(sd[keys[0]].shape == sd_hf[keys[1]].shape)\n",
    "            assert(keys[0] == keys[1])\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                sd[keys[0]].copy_(sd_hf[keys[1]])\n",
    "\n",
    "        return model\n",
    "\n",
    "@dataclass\n",
    "class RobertaConfig:\n",
    "    vocab_size = 50265\n",
    "    hidden_size = 768 \n",
    "    num_hidden_layers = 12\n",
    "    num_attention_heads = 12\n",
    "    intermediate_size = 3072\n",
    "    max_position_embeddings = 514\n",
    "    layer_norm_eps = 1e-12\n",
    "    \n",
    "    type_vocab_size = 1\n",
    "    pad_token_id = 1\n",
    "    bos_token_id = 0\n",
    "    eos_token_id = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9aab73f5-7fb7-4cc3-853a-791c7986586a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mechanic 9.1\n",
      " waiter 7.72\n",
      " butcher 7.64\n",
      " miner 4.75\n",
      " guard 4.22\n"
     ]
    }
   ],
   "source": [
    "model = RobertaMaskedLM.from_pretrained().to(device)\n",
    "model.eval()\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "# x = \"Criminals are most likely to have <mask> colored skin\"\n",
    "x = \"The man worked as a <mask>.\"\n",
    "x = torch.tensor(tokenizer.encode(x)).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(x)\n",
    "\n",
    "mask_token_index = (x == tokenizer.mask_token_id)[0].nonzero(as_tuple = True)[0]\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "\n",
    "probs = F.softmax(logits[0, mask_token_index], dim=1)\n",
    "\n",
    "topk = torch.topk(probs, 5)\n",
    "\n",
    "for i in range(topk.indices.shape[1]):\n",
    "    print(tokenizer.decode(topk.indices.squeeze()[i].item()), round(topk.values.squeeze()[i].item() * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b290c5f-4e9c-4169-9d5e-eca0bbff884c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50265, 768])\n",
      "torch.Size([50265, 768])\n"
     ]
    }
   ],
   "source": [
    "(lm.state_dict()['roberta.embeddings.word_embeddings.weight'] == lm.state_dict()['lm_head.decoder.weight']).all()\n",
    "print(lm.state_dict()['roberta.embeddings.word_embeddings.weight'].shape)\n",
    "print(lm.state_dict()['lm_head.decoder.weight'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd7b14c7-c3c9-4f9b-98eb-f239b6015c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForMaskedLM(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): RobertaLMHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
      "  )\n",
      ")\n",
      "roberta.embeddings.word_embeddings.weight\n",
      "roberta.embeddings.position_embeddings.weight\n",
      "roberta.embeddings.token_type_embeddings.weight\n",
      "roberta.embeddings.LayerNorm.weight\n",
      "roberta.embeddings.LayerNorm.bias\n",
      "roberta.encoder.layer.0.attention.self.query.weight\n",
      "roberta.encoder.layer.0.attention.self.query.bias\n",
      "roberta.encoder.layer.0.attention.self.key.weight\n",
      "roberta.encoder.layer.0.attention.self.key.bias\n",
      "roberta.encoder.layer.0.attention.self.value.weight\n",
      "roberta.encoder.layer.0.attention.self.value.bias\n",
      "roberta.encoder.layer.0.attention.output.dense.weight\n",
      "roberta.encoder.layer.0.attention.output.dense.bias\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.0.intermediate.dense.weight\n",
      "roberta.encoder.layer.0.intermediate.dense.bias\n",
      "roberta.encoder.layer.0.output.dense.weight\n",
      "roberta.encoder.layer.0.output.dense.bias\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias\n",
      "roberta.encoder.layer.1.attention.self.query.weight\n",
      "roberta.encoder.layer.1.attention.self.query.bias\n",
      "roberta.encoder.layer.1.attention.self.key.weight\n",
      "roberta.encoder.layer.1.attention.self.key.bias\n",
      "roberta.encoder.layer.1.attention.self.value.weight\n",
      "roberta.encoder.layer.1.attention.self.value.bias\n",
      "roberta.encoder.layer.1.attention.output.dense.weight\n",
      "roberta.encoder.layer.1.attention.output.dense.bias\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.1.intermediate.dense.weight\n",
      "roberta.encoder.layer.1.intermediate.dense.bias\n",
      "roberta.encoder.layer.1.output.dense.weight\n",
      "roberta.encoder.layer.1.output.dense.bias\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias\n",
      "roberta.encoder.layer.2.attention.self.query.weight\n",
      "roberta.encoder.layer.2.attention.self.query.bias\n",
      "roberta.encoder.layer.2.attention.self.key.weight\n",
      "roberta.encoder.layer.2.attention.self.key.bias\n",
      "roberta.encoder.layer.2.attention.self.value.weight\n",
      "roberta.encoder.layer.2.attention.self.value.bias\n",
      "roberta.encoder.layer.2.attention.output.dense.weight\n",
      "roberta.encoder.layer.2.attention.output.dense.bias\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.2.intermediate.dense.weight\n",
      "roberta.encoder.layer.2.intermediate.dense.bias\n",
      "roberta.encoder.layer.2.output.dense.weight\n",
      "roberta.encoder.layer.2.output.dense.bias\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.attention.self.query.weight\n",
      "roberta.encoder.layer.3.attention.self.query.bias\n",
      "roberta.encoder.layer.3.attention.self.key.weight\n",
      "roberta.encoder.layer.3.attention.self.key.bias\n",
      "roberta.encoder.layer.3.attention.self.value.weight\n",
      "roberta.encoder.layer.3.attention.self.value.bias\n",
      "roberta.encoder.layer.3.attention.output.dense.weight\n",
      "roberta.encoder.layer.3.attention.output.dense.bias\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.intermediate.dense.weight\n",
      "roberta.encoder.layer.3.intermediate.dense.bias\n",
      "roberta.encoder.layer.3.output.dense.weight\n",
      "roberta.encoder.layer.3.output.dense.bias\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.attention.self.query.weight\n",
      "roberta.encoder.layer.4.attention.self.query.bias\n",
      "roberta.encoder.layer.4.attention.self.key.weight\n",
      "roberta.encoder.layer.4.attention.self.key.bias\n",
      "roberta.encoder.layer.4.attention.self.value.weight\n",
      "roberta.encoder.layer.4.attention.self.value.bias\n",
      "roberta.encoder.layer.4.attention.output.dense.weight\n",
      "roberta.encoder.layer.4.attention.output.dense.bias\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.intermediate.dense.weight\n",
      "roberta.encoder.layer.4.intermediate.dense.bias\n",
      "roberta.encoder.layer.4.output.dense.weight\n",
      "roberta.encoder.layer.4.output.dense.bias\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.attention.self.query.weight\n",
      "roberta.encoder.layer.5.attention.self.query.bias\n",
      "roberta.encoder.layer.5.attention.self.key.weight\n",
      "roberta.encoder.layer.5.attention.self.key.bias\n",
      "roberta.encoder.layer.5.attention.self.value.weight\n",
      "roberta.encoder.layer.5.attention.self.value.bias\n",
      "roberta.encoder.layer.5.attention.output.dense.weight\n",
      "roberta.encoder.layer.5.attention.output.dense.bias\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.intermediate.dense.weight\n",
      "roberta.encoder.layer.5.intermediate.dense.bias\n",
      "roberta.encoder.layer.5.output.dense.weight\n",
      "roberta.encoder.layer.5.output.dense.bias\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.attention.self.query.weight\n",
      "roberta.encoder.layer.6.attention.self.query.bias\n",
      "roberta.encoder.layer.6.attention.self.key.weight\n",
      "roberta.encoder.layer.6.attention.self.key.bias\n",
      "roberta.encoder.layer.6.attention.self.value.weight\n",
      "roberta.encoder.layer.6.attention.self.value.bias\n",
      "roberta.encoder.layer.6.attention.output.dense.weight\n",
      "roberta.encoder.layer.6.attention.output.dense.bias\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.intermediate.dense.weight\n",
      "roberta.encoder.layer.6.intermediate.dense.bias\n",
      "roberta.encoder.layer.6.output.dense.weight\n",
      "roberta.encoder.layer.6.output.dense.bias\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.attention.self.query.weight\n",
      "roberta.encoder.layer.7.attention.self.query.bias\n",
      "roberta.encoder.layer.7.attention.self.key.weight\n",
      "roberta.encoder.layer.7.attention.self.key.bias\n",
      "roberta.encoder.layer.7.attention.self.value.weight\n",
      "roberta.encoder.layer.7.attention.self.value.bias\n",
      "roberta.encoder.layer.7.attention.output.dense.weight\n",
      "roberta.encoder.layer.7.attention.output.dense.bias\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.intermediate.dense.weight\n",
      "roberta.encoder.layer.7.intermediate.dense.bias\n",
      "roberta.encoder.layer.7.output.dense.weight\n",
      "roberta.encoder.layer.7.output.dense.bias\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.attention.self.query.weight\n",
      "roberta.encoder.layer.8.attention.self.query.bias\n",
      "roberta.encoder.layer.8.attention.self.key.weight\n",
      "roberta.encoder.layer.8.attention.self.key.bias\n",
      "roberta.encoder.layer.8.attention.self.value.weight\n",
      "roberta.encoder.layer.8.attention.self.value.bias\n",
      "roberta.encoder.layer.8.attention.output.dense.weight\n",
      "roberta.encoder.layer.8.attention.output.dense.bias\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.intermediate.dense.weight\n",
      "roberta.encoder.layer.8.intermediate.dense.bias\n",
      "roberta.encoder.layer.8.output.dense.weight\n",
      "roberta.encoder.layer.8.output.dense.bias\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.attention.self.query.weight\n",
      "roberta.encoder.layer.9.attention.self.query.bias\n",
      "roberta.encoder.layer.9.attention.self.key.weight\n",
      "roberta.encoder.layer.9.attention.self.key.bias\n",
      "roberta.encoder.layer.9.attention.self.value.weight\n",
      "roberta.encoder.layer.9.attention.self.value.bias\n",
      "roberta.encoder.layer.9.attention.output.dense.weight\n",
      "roberta.encoder.layer.9.attention.output.dense.bias\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.intermediate.dense.weight\n",
      "roberta.encoder.layer.9.intermediate.dense.bias\n",
      "roberta.encoder.layer.9.output.dense.weight\n",
      "roberta.encoder.layer.9.output.dense.bias\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.attention.self.query.weight\n",
      "roberta.encoder.layer.10.attention.self.query.bias\n",
      "roberta.encoder.layer.10.attention.self.key.weight\n",
      "roberta.encoder.layer.10.attention.self.key.bias\n",
      "roberta.encoder.layer.10.attention.self.value.weight\n",
      "roberta.encoder.layer.10.attention.self.value.bias\n",
      "roberta.encoder.layer.10.attention.output.dense.weight\n",
      "roberta.encoder.layer.10.attention.output.dense.bias\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.intermediate.dense.weight\n",
      "roberta.encoder.layer.10.intermediate.dense.bias\n",
      "roberta.encoder.layer.10.output.dense.weight\n",
      "roberta.encoder.layer.10.output.dense.bias\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.attention.self.query.weight\n",
      "roberta.encoder.layer.11.attention.self.query.bias\n",
      "roberta.encoder.layer.11.attention.self.key.weight\n",
      "roberta.encoder.layer.11.attention.self.key.bias\n",
      "roberta.encoder.layer.11.attention.self.value.weight\n",
      "roberta.encoder.layer.11.attention.self.value.bias\n",
      "roberta.encoder.layer.11.attention.output.dense.weight\n",
      "roberta.encoder.layer.11.attention.output.dense.bias\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.intermediate.dense.weight\n",
      "roberta.encoder.layer.11.intermediate.dense.bias\n",
      "roberta.encoder.layer.11.output.dense.weight\n",
      "roberta.encoder.layer.11.output.dense.bias\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias\n",
      "lm_head.bias\n",
      "lm_head.dense.weight\n",
      "lm_head.dense.bias\n",
      "lm_head.layer_norm.weight\n",
      "lm_head.layer_norm.bias\n",
      "lm_head.decoder.weight\n",
      "lm_head.decoder.bias\n"
     ]
    }
   ],
   "source": [
    "lm = RobertaForMaskedLM.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "keys = lm.state_dict().keys()\n",
    "print(lm)\n",
    "for key in keys:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16859306-e6b3-4ac7-ac00-a24e1ff6ad34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaMaskedLM(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(512, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELU(approximate='none')\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): RobertaLMHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (gelu): GELU(approximate='none')\n",
      "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "config = RobertaConfig()\n",
    "model = RobertaMaskedLM(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5adf5402-25dc-4359-9566-bfd28a19485a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForMaskedLM(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): RobertaLMHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForMaskedLM.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373c54bd-ef7e-4579-9bb1-54d0a47107ee",
   "metadata": {},
   "source": [
    "## Roberta Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "3295324b-9cdf-4e2a-89ce-b85281992b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.intermediate = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.output = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.intermediate(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.output(x)\n",
    "        return x \n",
    "\n",
    "class FullSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.key = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.value = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        \n",
    "        self.output = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        \n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        B, T, C = x.size()\n",
    "        # (batch_size, max_len, hidden_size) \n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        q = q.view(B, T, self.num_attention_heads, C // self.num_attention_heads).transpose(1,2) # (B, num_head, T, head_size)\n",
    "        k = k.view(B, T, self.num_attention_heads, C // self.num_attention_heads).transpose(1,2)\n",
    "        v = v.view(B, T, self.num_attention_heads, C // self.num_attention_heads).transpose(1,2)\n",
    "\n",
    "        # Calculate attention scores \n",
    "        attn = (q @ k.transpose(2, 3)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = F.softmax(attn, dim = -1)\n",
    "\n",
    "        y = attn @ v # (B, nh, T, T) x (B, nh, T, hs) = (B, nh, T, hs)\n",
    "        y = y.transpose(1,2).contiguous().view(B,T,C) # Concat head outputs\n",
    "\n",
    "        # Project\n",
    "        y = self.output(y)        \n",
    "        return y\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        # self.attention = FullSelfAttention(config)\n",
    "        self.attention = RobertaSdpaSelfAttention(config)\n",
    "        self.ln_1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.mlp = MLP(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.hidden_size)\n",
    "        \n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = x + self.ln_1(self.attention(x, mask)) \n",
    "        x = x + self.ln_2(self.mlp(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class RobertaConfig:\n",
    "    vocab_size = 50265\n",
    "    hidden_size = 768 \n",
    "    num_hidden_layers = 12\n",
    "    num_attention_heads = 12\n",
    "    intermediate_size = 3072\n",
    "    max_position_embeddings = 512\n",
    "    \n",
    "    type_vocab_size = 1\n",
    "    pad_token_id = 1\n",
    "    bos_token_id = 0\n",
    "    eos_token_id = 2\n",
    "    \n",
    "\n",
    "\n",
    "class Roberta(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.embeddings = nn.ModuleDict(dict(\n",
    "            word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx = config.pad_token_id),\n",
    "            position_embeddings = nn.Embedding(config.max_position_embeddings + 2, config.hidden_size, padding_idx = config.pad_token_id),\n",
    "            token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size),\n",
    "            LayerNorm = nn.LayerNorm(config.hidden_size)\n",
    "        ))\n",
    "        \n",
    "        self.encoder = nn.ModuleDict(dict(\n",
    "            layer = nn.ModuleList([EncoderBlock(config) for _ in range(config.num_hidden_layers)])\n",
    "        ))\n",
    "        \n",
    "        self.lm_head = nn.ModuleDict(dict(\n",
    "            dense = nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            gelu = nn.GELU(),\n",
    "            LayerNorm = nn.LayerNorm(config.hidden_size),\n",
    "            decoder = nn.Linear(config.hidden_size, config.vocab_size),\n",
    "        ))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        # attention masking for padded token\n",
    "        # (batch_size, 1, seq_len, seq_len)\n",
    "        mask = (x >= 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1).to(x.device)\n",
    "\n",
    "        # Token embeddings\n",
    "        tok_emb = self.embeddings.word_embeddings(x)\n",
    "\n",
    "        # Positional embeddings \n",
    "        pos_mask = x.ne(self.config.pad_token_id).int()\n",
    "        indices = (((torch.cumsum(pos_mask,dim=1)).type_as(pos_mask))* pos_mask) + self.config.pad_token_id\n",
    "        pos_emb = self.embeddings.position_embeddings(indices)\n",
    "\n",
    "        # pos = torch.arange(0, T, dtype = torch.long, device = x.device)\n",
    "        # pos_emb = self.embeddings.position_embeddings(pos)\n",
    "        \n",
    "\n",
    "        \n",
    "        # Token Type embeddings\n",
    "        # typ = torch.zeros((1,T), dtype = torch.long, device = x.device)\n",
    "        # type_emb = self.embeddings.token_type_embeddings(typ)\n",
    "        \n",
    "\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        x = self.embeddings.LayerNorm(x)\n",
    "        \n",
    "        # Pass batch through transformer \n",
    "        for block in self.encoder.layer:\n",
    "            x = block(x, mask)\n",
    "\n",
    "        # Pass through prediction head\n",
    "        x = self.lm_head.dense(x)\n",
    "        x = self.lm_head.gelu(x)\n",
    "        x = self.lm_head.LayerNorm(x)\n",
    "        x = self.lm_head.decoder(x)\n",
    "\n",
    "        return x\n",
    "            \n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type = \"FacebookAI/roberta-base\"):\n",
    "        \"\"\" Loading pretrained Roberta weights from hugging face \"\"\"\n",
    "        print(\"loading weights for %s\" % model_type)\n",
    "\n",
    "        # Random init of model\n",
    "        config = RobertaConfig()\n",
    "        model = Roberta(config)\n",
    "        \n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "\n",
    "        # Init a Roberta from hugging face \n",
    "        model_hf = RobertaForMaskedLM.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "        sd_hf_keys = [k for k in sd_hf.keys() if not k.endswith('lm_head.bias')]\n",
    "        # Copy over weights. State Dicts are currently in same order, so I can just blind copy \n",
    "        for keys in zip(sd_keys, sd_hf_keys):\n",
    "            # print(sd[keys[0]].shape)\n",
    "            # print(sd_hf[keys[1]].shape)\n",
    "            \n",
    "            assert(sd[keys[0]].shape == sd_hf[keys[1]].shape)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                sd[keys[0]].copy_(sd_hf[keys[1]])\n",
    "\n",
    "        return model    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3441be52-58e1-4d2b-9835-280aaa653bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "422bf265-85ba-4edb-a147-399bae279366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights for FacebookAI/roberta-base\n",
      "loading weights for FacebookAI/roberta-base\n",
      "loading weights for FacebookAI/roberta-base\n",
      "loading weights for FacebookAI/roberta-base\n",
      "loading weights for FacebookAI/roberta-base\n",
      "loading weights for FacebookAI/roberta-base\n",
      "loading weights for FacebookAI/roberta-base\n",
      "loading weights for FacebookAI/roberta-base\n",
      "loading weights for FacebookAI/roberta-base\n",
      "loading weights for FacebookAI/roberta-base\n",
      "loading weights for FacebookAI/roberta-base\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[394], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mRobertaForMaskedLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m RobertaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFacebookAI/roberta-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe capital of <mask> is Paris\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[381], line 58\u001b[0m, in \u001b[0;36mRobertaForMaskedLM.from_pretrained\u001b[0;34m(cls, model_type)\u001b[0m\n\u001b[1;32m     55\u001b[0m sd_keys \u001b[38;5;241m=\u001b[39m sd\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Init a Roberta from hugging face \u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m model_hf \u001b[38;5;241m=\u001b[39m \u001b[43mRobertaForMaskedLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m sd_hf \u001b[38;5;241m=\u001b[39m model_hf\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[1;32m     60\u001b[0m sd_hf_keys \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m sd_hf\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m k\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlm_head.bias\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "Cell \u001b[0;32mIn[381], line 58\u001b[0m, in \u001b[0;36mRobertaForMaskedLM.from_pretrained\u001b[0;34m(cls, model_type)\u001b[0m\n\u001b[1;32m     55\u001b[0m sd_keys \u001b[38;5;241m=\u001b[39m sd\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Init a Roberta from hugging face \u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m model_hf \u001b[38;5;241m=\u001b[39m \u001b[43mRobertaForMaskedLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m sd_hf \u001b[38;5;241m=\u001b[39m model_hf\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[1;32m     60\u001b[0m sd_hf_keys \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m sd_hf\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m k\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlm_head.bias\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "    \u001b[0;31m[... skipping similar frames: RobertaForMaskedLM.from_pretrained at line 58 (7 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[381], line 58\u001b[0m, in \u001b[0;36mRobertaForMaskedLM.from_pretrained\u001b[0;34m(cls, model_type)\u001b[0m\n\u001b[1;32m     55\u001b[0m sd_keys \u001b[38;5;241m=\u001b[39m sd\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Init a Roberta from hugging face \u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m model_hf \u001b[38;5;241m=\u001b[39m \u001b[43mRobertaForMaskedLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m sd_hf \u001b[38;5;241m=\u001b[39m model_hf\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[1;32m     60\u001b[0m sd_hf_keys \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m sd_hf\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m k\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlm_head.bias\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "Cell \u001b[0;32mIn[381], line 52\u001b[0m, in \u001b[0;36mRobertaForMaskedLM.from_pretrained\u001b[0;34m(cls, model_type)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Random init of model\u001b[39;00m\n\u001b[1;32m     51\u001b[0m config \u001b[38;5;241m=\u001b[39m RobertaConfig()\n\u001b[0;32m---> 52\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mRobertaForMaskedLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m sd \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[1;32m     55\u001b[0m sd_keys \u001b[38;5;241m=\u001b[39m sd\u001b[38;5;241m.\u001b[39mkeys()\n",
      "Cell \u001b[0;32mIn[381], line 6\u001b[0m, in \u001b[0;36mRobertaForMaskedLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroberta \u001b[38;5;241m=\u001b[39m \u001b[43mRobertaModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m RobertaLMHead(config)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n",
      "Cell \u001b[0;32mIn[391], line 285\u001b[0m, in \u001b[0;36mRobertaModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 285\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m \u001b[43mRobertaEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m RobertaEncoder(config)\n",
      "Cell \u001b[0;32mIn[391], line 9\u001b[0m, in \u001b[0;36mRobertaEmbeddings.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mmax_position_embeddings, config\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mtype_vocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch-gpu/lib/python3.13/site-packages/torch/nn/modules/sparse.py:170\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\n\u001b[1;32m    167\u001b[0m         torch\u001b[38;5;241m.\u001b[39mempty((num_embeddings, embedding_dim), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs),\n\u001b[1;32m    168\u001b[0m         requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m _freeze,\n\u001b[1;32m    169\u001b[0m     )\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_weight\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m [\n\u001b[1;32m    173\u001b[0m         num_embeddings,\n\u001b[1;32m    174\u001b[0m         embedding_dim,\n\u001b[1;32m    175\u001b[0m     ], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of weight does not match num_embeddings and embedding_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch-gpu/lib/python3.13/site-packages/torch/nn/modules/sparse.py:181\u001b[0m, in \u001b[0;36mEmbedding.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 181\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fill_padding_idx_with_zero()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch-gpu/lib/python3.13/site-packages/torch/nn/init.py:193\u001b[0m, in \u001b[0;36mnormal_\u001b[0;34m(tensor, mean, std, generator)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhas_torch_function_variadic(tensor):\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhandle_torch_function(\n\u001b[1;32m    191\u001b[0m         normal_, (tensor,), tensor\u001b[38;5;241m=\u001b[39mtensor, mean\u001b[38;5;241m=\u001b[39mmean, std\u001b[38;5;241m=\u001b[39mstd, generator\u001b[38;5;241m=\u001b[39mgenerator\n\u001b[1;32m    192\u001b[0m     )\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_no_grad_normal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch-gpu/lib/python3.13/site-packages/torch/nn/init.py:22\u001b[0m, in \u001b[0;36m_no_grad_normal_\u001b[0;34m(tensor, mean, std, generator)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_no_grad_normal_\u001b[39m(tensor, mean, std, generator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 22\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = RobertaMaskedLM.from_pretrained().to(device)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "x = \"The capital of <mask> is Paris\"\n",
    "x = torch.tensor(tokenizer.encode(x)).unsqueeze(0).to(device)\n",
    "print(x)\n",
    "with torch.no_grad():\n",
    "    logits = model(x)\n",
    "\n",
    "mask_token_index = (x == tokenizer.mask_token_id)[0].nonzero(as_tuple = True)[0]\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "print(tokenizer.decode(predicted_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "85783036-7745-4da3-abc6-19515c6fb2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",,,,,,,,\n",
      "tensor([ 0.6646, -5.2787,  9.3434,  ..., -3.4416, -4.1338,  4.4339],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "o = logits[0].argmax(axis=-1)\n",
    "print(tokenizer.decode(o))\n",
    "print(logits[0, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "feffea3f-a86d-4659-9781-8f87fc36fc1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RobertaLMHeadModel' from 'transformers' (/opt/anaconda3/envs/pytorch-gpu/lib/python3.13/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[346], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RobertaLMHeadModel\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'RobertaLMHeadModel' from 'transformers' (/opt/anaconda3/envs/pytorch-gpu/lib/python3.13/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaLMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "20a55635-208e-460b-874e-25a02e3dea02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   133,   812,     9,  1470,    16, 50264,     4,     1,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "torch.Size([1, 10, 50265])\n",
      " Paris\n",
      "MaskedLMOutput(loss=tensor(0.0990, grad_fn=<NllLossBackward0>), logits=tensor([[[34.2751, -3.7783, 18.3324,  ...,  2.7908,  5.3139, 11.8587],\n",
      "         [ 8.6809, -2.8665, 18.9842,  ...,  2.8312,  4.0936,  9.4246],\n",
      "         [-3.3497, -4.3248,  8.5588,  ..., -1.9859, -2.6968,  0.3392],\n",
      "         ...,\n",
      "         [21.2609, -4.2944, 19.6318,  ...,  0.9557,  3.3131,  8.0253],\n",
      "         [10.4854, -4.2014, 28.6527,  ..., -1.6521, -3.9379,  8.8991],\n",
      "         [11.1121, -3.5715, 31.1623,  ...,  1.5217, -0.4953,  9.6180]]],\n",
      "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, RobertaForMaskedLM, RobertaForCasualLM\n",
    "import torch\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "model = RobertaForMaskedLM.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "inputs = tokenizer(\"The capital of France is <mask>.<pad>\", return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "print(logits.shape)\n",
    "# retrieve index of <mask>\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "print(tokenizer.decode(predicted_token_id))\n",
    "\n",
    "labels = tokenizer(\"The capital of France is Paris.<pad>\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "# mask labels of non-<mask> tokens\n",
    "labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
    "\n",
    "outputs = model(**inputs, labels=labels)\n",
    "print(outputs)\n",
    "round(outputs.loss.item(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "7dbf0ae7-a061-44c2-9c42-2cb0d8bdaa49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights for FacebookAI/roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "model = Roberta.from_pretrained().to(device)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "x = \"The capital of France is <mask>.<pad>\"\n",
    "x = torch.tensor(tokenizer.encode(x)).unsqueeze(0).to(device)\n",
    "\n",
    "logits = model.forward(x)\n",
    "print(logits.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "846674b5-5102-4c93-b1ce-c219114a78ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2201)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0,6].argmax(axis=-1)\n",
    "print(tokenizer.decoder(predicted_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "28b4af17-63d0-4466-9d3f-e289f3179089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n",
    "    \"\"\"\n",
    "    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n",
    "    are ignored. This is modified from fairseq's `utils.make_positions`.\n",
    "\n",
    "    Args:\n",
    "        x: torch.Tensor x:\n",
    "\n",
    "    Returns: torch.Tensor\n",
    "    \"\"\"\n",
    "    # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n",
    "    mask = input_ids.ne(padding_idx).int()\n",
    "    incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n",
    "    return incremental_indices.long() + padding_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "8655e9fc-a709-49b8-8ff6-db39a89c025b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 0, 4, 5, 6],\n",
      "        [1, 2, 3, 0, 4, 5, 6]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[0,2,3,4,1,1,1],[0,5,2,4,9,1,1]])\n",
    "mask = x.ne(4).int()\n",
    "\n",
    "ind = ((torch.cumsum(mask,dim=1)).type_as(mask))* mask \n",
    "print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "cefa99e0-1f05-409e-8a0b-d2e43739d078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights for FacebookAI/roberta-base\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RobertaConfig' object has no attribute 'layer_norm_eps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[384], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mRobertaForMaskedLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# x = torch.tensor([[1, 2, 3, 4, 5], [5,6,7,0, 0], [9,9,0,0,0]])\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# print(model.forward(x))\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[381], line 52\u001b[0m, in \u001b[0;36mRobertaForMaskedLM.from_pretrained\u001b[0;34m(cls, model_type)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Random init of model\u001b[39;00m\n\u001b[1;32m     51\u001b[0m config \u001b[38;5;241m=\u001b[39m RobertaConfig()\n\u001b[0;32m---> 52\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mRobertaForMaskedLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m sd \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[1;32m     55\u001b[0m sd_keys \u001b[38;5;241m=\u001b[39m sd\u001b[38;5;241m.\u001b[39mkeys()\n",
      "Cell \u001b[0;32mIn[381], line 6\u001b[0m, in \u001b[0;36mRobertaForMaskedLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroberta \u001b[38;5;241m=\u001b[39m \u001b[43mRobertaModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m RobertaLMHead(config)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n",
      "Cell \u001b[0;32mIn[380], line 285\u001b[0m, in \u001b[0;36mRobertaModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 285\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m \u001b[43mRobertaEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m RobertaEncoder(config)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[380], line 15\u001b[0m, in \u001b[0;36mRobertaEmbeddings.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mtype_vocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# any TensorFlow checkpoint file\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm_eps\u001b[49m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# position_ids (1, len position emb) is contiguous in memory and exported when serialized\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabsolute\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RobertaConfig' object has no attribute 'layer_norm_eps'"
     ]
    }
   ],
   "source": [
    "model = RobertaForMaskedLM.from_pretrained()\n",
    "print(model)\n",
    "\n",
    "# x = torch.tensor([[1, 2, 3, 4, 5], [5,6,7,0, 0], [9,9,0,0,0]])\n",
    "# print(model.forward(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "11438ad2-3e42-4ec6-a34b-3846bf341850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "model_hf = RobertaModel.from_pretrained(\"FacebookAI/roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b288df6a-109c-4038-91b6-8c1994ec44d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 133, 812, 9, 1470, 16, 50264, 4, 1, 1, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer('The capital of France is <mask>.<pad><pad>')\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "751ca591-3fc2-4c04-bf71-7e976e55455f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForMaskedLM(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): RobertaLMHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model_hf = RobertaForMaskedLM.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "from transformers import AutoTokenizer, RobertaForMaskedLM, RobertaForCausalLM, RobertaForSequenceClassification, RobertaForQuestionAnswering\n",
    "\n",
    "model_hf = RobertaForMaskedLM.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "print(model_hf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "573ab34f-615e-4ad6-94e7-9b63a421b6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.embeddings.word_embeddings.weight\n",
      "roberta.embeddings.position_embeddings.weight\n",
      "roberta.embeddings.token_type_embeddings.weight\n",
      "roberta.embeddings.LayerNorm.weight\n",
      "roberta.embeddings.LayerNorm.bias\n",
      "roberta.encoder.layer.0.attention.self.query.weight\n",
      "roberta.encoder.layer.0.attention.self.query.bias\n",
      "roberta.encoder.layer.0.attention.self.key.weight\n",
      "roberta.encoder.layer.0.attention.self.key.bias\n",
      "roberta.encoder.layer.0.attention.self.value.weight\n",
      "roberta.encoder.layer.0.attention.self.value.bias\n",
      "roberta.encoder.layer.0.attention.output.dense.weight\n",
      "roberta.encoder.layer.0.attention.output.dense.bias\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.0.intermediate.dense.weight\n",
      "roberta.encoder.layer.0.intermediate.dense.bias\n",
      "roberta.encoder.layer.0.output.dense.weight\n",
      "roberta.encoder.layer.0.output.dense.bias\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias\n",
      "roberta.encoder.layer.1.attention.self.query.weight\n",
      "roberta.encoder.layer.1.attention.self.query.bias\n",
      "roberta.encoder.layer.1.attention.self.key.weight\n",
      "roberta.encoder.layer.1.attention.self.key.bias\n",
      "roberta.encoder.layer.1.attention.self.value.weight\n",
      "roberta.encoder.layer.1.attention.self.value.bias\n",
      "roberta.encoder.layer.1.attention.output.dense.weight\n",
      "roberta.encoder.layer.1.attention.output.dense.bias\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.1.intermediate.dense.weight\n",
      "roberta.encoder.layer.1.intermediate.dense.bias\n",
      "roberta.encoder.layer.1.output.dense.weight\n",
      "roberta.encoder.layer.1.output.dense.bias\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias\n",
      "roberta.encoder.layer.2.attention.self.query.weight\n",
      "roberta.encoder.layer.2.attention.self.query.bias\n",
      "roberta.encoder.layer.2.attention.self.key.weight\n",
      "roberta.encoder.layer.2.attention.self.key.bias\n",
      "roberta.encoder.layer.2.attention.self.value.weight\n",
      "roberta.encoder.layer.2.attention.self.value.bias\n",
      "roberta.encoder.layer.2.attention.output.dense.weight\n",
      "roberta.encoder.layer.2.attention.output.dense.bias\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.2.intermediate.dense.weight\n",
      "roberta.encoder.layer.2.intermediate.dense.bias\n",
      "roberta.encoder.layer.2.output.dense.weight\n",
      "roberta.encoder.layer.2.output.dense.bias\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.attention.self.query.weight\n",
      "roberta.encoder.layer.3.attention.self.query.bias\n",
      "roberta.encoder.layer.3.attention.self.key.weight\n",
      "roberta.encoder.layer.3.attention.self.key.bias\n",
      "roberta.encoder.layer.3.attention.self.value.weight\n",
      "roberta.encoder.layer.3.attention.self.value.bias\n",
      "roberta.encoder.layer.3.attention.output.dense.weight\n",
      "roberta.encoder.layer.3.attention.output.dense.bias\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.intermediate.dense.weight\n",
      "roberta.encoder.layer.3.intermediate.dense.bias\n",
      "roberta.encoder.layer.3.output.dense.weight\n",
      "roberta.encoder.layer.3.output.dense.bias\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.attention.self.query.weight\n",
      "roberta.encoder.layer.4.attention.self.query.bias\n",
      "roberta.encoder.layer.4.attention.self.key.weight\n",
      "roberta.encoder.layer.4.attention.self.key.bias\n",
      "roberta.encoder.layer.4.attention.self.value.weight\n",
      "roberta.encoder.layer.4.attention.self.value.bias\n",
      "roberta.encoder.layer.4.attention.output.dense.weight\n",
      "roberta.encoder.layer.4.attention.output.dense.bias\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.intermediate.dense.weight\n",
      "roberta.encoder.layer.4.intermediate.dense.bias\n",
      "roberta.encoder.layer.4.output.dense.weight\n",
      "roberta.encoder.layer.4.output.dense.bias\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.attention.self.query.weight\n",
      "roberta.encoder.layer.5.attention.self.query.bias\n",
      "roberta.encoder.layer.5.attention.self.key.weight\n",
      "roberta.encoder.layer.5.attention.self.key.bias\n",
      "roberta.encoder.layer.5.attention.self.value.weight\n",
      "roberta.encoder.layer.5.attention.self.value.bias\n",
      "roberta.encoder.layer.5.attention.output.dense.weight\n",
      "roberta.encoder.layer.5.attention.output.dense.bias\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.intermediate.dense.weight\n",
      "roberta.encoder.layer.5.intermediate.dense.bias\n",
      "roberta.encoder.layer.5.output.dense.weight\n",
      "roberta.encoder.layer.5.output.dense.bias\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.attention.self.query.weight\n",
      "roberta.encoder.layer.6.attention.self.query.bias\n",
      "roberta.encoder.layer.6.attention.self.key.weight\n",
      "roberta.encoder.layer.6.attention.self.key.bias\n",
      "roberta.encoder.layer.6.attention.self.value.weight\n",
      "roberta.encoder.layer.6.attention.self.value.bias\n",
      "roberta.encoder.layer.6.attention.output.dense.weight\n",
      "roberta.encoder.layer.6.attention.output.dense.bias\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.intermediate.dense.weight\n",
      "roberta.encoder.layer.6.intermediate.dense.bias\n",
      "roberta.encoder.layer.6.output.dense.weight\n",
      "roberta.encoder.layer.6.output.dense.bias\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.attention.self.query.weight\n",
      "roberta.encoder.layer.7.attention.self.query.bias\n",
      "roberta.encoder.layer.7.attention.self.key.weight\n",
      "roberta.encoder.layer.7.attention.self.key.bias\n",
      "roberta.encoder.layer.7.attention.self.value.weight\n",
      "roberta.encoder.layer.7.attention.self.value.bias\n",
      "roberta.encoder.layer.7.attention.output.dense.weight\n",
      "roberta.encoder.layer.7.attention.output.dense.bias\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.intermediate.dense.weight\n",
      "roberta.encoder.layer.7.intermediate.dense.bias\n",
      "roberta.encoder.layer.7.output.dense.weight\n",
      "roberta.encoder.layer.7.output.dense.bias\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.attention.self.query.weight\n",
      "roberta.encoder.layer.8.attention.self.query.bias\n",
      "roberta.encoder.layer.8.attention.self.key.weight\n",
      "roberta.encoder.layer.8.attention.self.key.bias\n",
      "roberta.encoder.layer.8.attention.self.value.weight\n",
      "roberta.encoder.layer.8.attention.self.value.bias\n",
      "roberta.encoder.layer.8.attention.output.dense.weight\n",
      "roberta.encoder.layer.8.attention.output.dense.bias\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.intermediate.dense.weight\n",
      "roberta.encoder.layer.8.intermediate.dense.bias\n",
      "roberta.encoder.layer.8.output.dense.weight\n",
      "roberta.encoder.layer.8.output.dense.bias\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.attention.self.query.weight\n",
      "roberta.encoder.layer.9.attention.self.query.bias\n",
      "roberta.encoder.layer.9.attention.self.key.weight\n",
      "roberta.encoder.layer.9.attention.self.key.bias\n",
      "roberta.encoder.layer.9.attention.self.value.weight\n",
      "roberta.encoder.layer.9.attention.self.value.bias\n",
      "roberta.encoder.layer.9.attention.output.dense.weight\n",
      "roberta.encoder.layer.9.attention.output.dense.bias\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.intermediate.dense.weight\n",
      "roberta.encoder.layer.9.intermediate.dense.bias\n",
      "roberta.encoder.layer.9.output.dense.weight\n",
      "roberta.encoder.layer.9.output.dense.bias\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.attention.self.query.weight\n",
      "roberta.encoder.layer.10.attention.self.query.bias\n",
      "roberta.encoder.layer.10.attention.self.key.weight\n",
      "roberta.encoder.layer.10.attention.self.key.bias\n",
      "roberta.encoder.layer.10.attention.self.value.weight\n",
      "roberta.encoder.layer.10.attention.self.value.bias\n",
      "roberta.encoder.layer.10.attention.output.dense.weight\n",
      "roberta.encoder.layer.10.attention.output.dense.bias\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.intermediate.dense.weight\n",
      "roberta.encoder.layer.10.intermediate.dense.bias\n",
      "roberta.encoder.layer.10.output.dense.weight\n",
      "roberta.encoder.layer.10.output.dense.bias\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.attention.self.query.weight\n",
      "roberta.encoder.layer.11.attention.self.query.bias\n",
      "roberta.encoder.layer.11.attention.self.key.weight\n",
      "roberta.encoder.layer.11.attention.self.key.bias\n",
      "roberta.encoder.layer.11.attention.self.value.weight\n",
      "roberta.encoder.layer.11.attention.self.value.bias\n",
      "roberta.encoder.layer.11.attention.output.dense.weight\n",
      "roberta.encoder.layer.11.attention.output.dense.bias\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.intermediate.dense.weight\n",
      "roberta.encoder.layer.11.intermediate.dense.bias\n",
      "roberta.encoder.layer.11.output.dense.weight\n",
      "roberta.encoder.layer.11.output.dense.bias\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias\n",
      "lm_head.bias\n",
      "lm_head.dense.weight\n",
      "lm_head.dense.bias\n",
      "lm_head.layer_norm.weight\n",
      "lm_head.layer_norm.bias\n",
      "lm_head.decoder.weight\n",
      "lm_head.decoder.bias\n"
     ]
    }
   ],
   "source": [
    "for k in model_hf.state_dict().keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "85c281e7-ee83-43c3-bcfa-1bfefa1caea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea1577df-bbbb-46b8-816f-c2c330c42221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight\n",
      "embeddings.position_embeddings.weight\n",
      "embeddings.token_type_embeddings.weight\n",
      "embeddings.LayerNorm.weight\n",
      "embeddings.LayerNorm.bias\n",
      "encoder.layer.0.attention.self.query.weight\n",
      "encoder.layer.0.attention.self.query.bias\n",
      "encoder.layer.0.attention.self.key.weight\n",
      "encoder.layer.0.attention.self.key.bias\n",
      "encoder.layer.0.attention.self.value.weight\n",
      "encoder.layer.0.attention.self.value.bias\n",
      "encoder.layer.0.attention.output.dense.weight\n",
      "encoder.layer.0.attention.output.dense.bias\n",
      "encoder.layer.0.attention.output.LayerNorm.weight\n",
      "encoder.layer.0.attention.output.LayerNorm.bias\n",
      "encoder.layer.0.intermediate.dense.weight\n",
      "encoder.layer.0.intermediate.dense.bias\n",
      "encoder.layer.0.output.dense.weight\n",
      "encoder.layer.0.output.dense.bias\n",
      "encoder.layer.0.output.LayerNorm.weight\n",
      "encoder.layer.0.output.LayerNorm.bias\n",
      "encoder.layer.1.attention.self.query.weight\n",
      "encoder.layer.1.attention.self.query.bias\n",
      "encoder.layer.1.attention.self.key.weight\n",
      "encoder.layer.1.attention.self.key.bias\n",
      "encoder.layer.1.attention.self.value.weight\n",
      "encoder.layer.1.attention.self.value.bias\n",
      "encoder.layer.1.attention.output.dense.weight\n",
      "encoder.layer.1.attention.output.dense.bias\n",
      "encoder.layer.1.attention.output.LayerNorm.weight\n",
      "encoder.layer.1.attention.output.LayerNorm.bias\n",
      "encoder.layer.1.intermediate.dense.weight\n",
      "encoder.layer.1.intermediate.dense.bias\n",
      "encoder.layer.1.output.dense.weight\n",
      "encoder.layer.1.output.dense.bias\n",
      "encoder.layer.1.output.LayerNorm.weight\n",
      "encoder.layer.1.output.LayerNorm.bias\n",
      "encoder.layer.2.attention.self.query.weight\n",
      "encoder.layer.2.attention.self.query.bias\n",
      "encoder.layer.2.attention.self.key.weight\n",
      "encoder.layer.2.attention.self.key.bias\n",
      "encoder.layer.2.attention.self.value.weight\n",
      "encoder.layer.2.attention.self.value.bias\n",
      "encoder.layer.2.attention.output.dense.weight\n",
      "encoder.layer.2.attention.output.dense.bias\n",
      "encoder.layer.2.attention.output.LayerNorm.weight\n",
      "encoder.layer.2.attention.output.LayerNorm.bias\n",
      "encoder.layer.2.intermediate.dense.weight\n",
      "encoder.layer.2.intermediate.dense.bias\n",
      "encoder.layer.2.output.dense.weight\n",
      "encoder.layer.2.output.dense.bias\n",
      "encoder.layer.2.output.LayerNorm.weight\n",
      "encoder.layer.2.output.LayerNorm.bias\n",
      "encoder.layer.3.attention.self.query.weight\n",
      "encoder.layer.3.attention.self.query.bias\n",
      "encoder.layer.3.attention.self.key.weight\n",
      "encoder.layer.3.attention.self.key.bias\n",
      "encoder.layer.3.attention.self.value.weight\n",
      "encoder.layer.3.attention.self.value.bias\n",
      "encoder.layer.3.attention.output.dense.weight\n",
      "encoder.layer.3.attention.output.dense.bias\n",
      "encoder.layer.3.attention.output.LayerNorm.weight\n",
      "encoder.layer.3.attention.output.LayerNorm.bias\n",
      "encoder.layer.3.intermediate.dense.weight\n",
      "encoder.layer.3.intermediate.dense.bias\n",
      "encoder.layer.3.output.dense.weight\n",
      "encoder.layer.3.output.dense.bias\n",
      "encoder.layer.3.output.LayerNorm.weight\n",
      "encoder.layer.3.output.LayerNorm.bias\n",
      "encoder.layer.4.attention.self.query.weight\n",
      "encoder.layer.4.attention.self.query.bias\n",
      "encoder.layer.4.attention.self.key.weight\n",
      "encoder.layer.4.attention.self.key.bias\n",
      "encoder.layer.4.attention.self.value.weight\n",
      "encoder.layer.4.attention.self.value.bias\n",
      "encoder.layer.4.attention.output.dense.weight\n",
      "encoder.layer.4.attention.output.dense.bias\n",
      "encoder.layer.4.attention.output.LayerNorm.weight\n",
      "encoder.layer.4.attention.output.LayerNorm.bias\n",
      "encoder.layer.4.intermediate.dense.weight\n",
      "encoder.layer.4.intermediate.dense.bias\n",
      "encoder.layer.4.output.dense.weight\n",
      "encoder.layer.4.output.dense.bias\n",
      "encoder.layer.4.output.LayerNorm.weight\n",
      "encoder.layer.4.output.LayerNorm.bias\n",
      "encoder.layer.5.attention.self.query.weight\n",
      "encoder.layer.5.attention.self.query.bias\n",
      "encoder.layer.5.attention.self.key.weight\n",
      "encoder.layer.5.attention.self.key.bias\n",
      "encoder.layer.5.attention.self.value.weight\n",
      "encoder.layer.5.attention.self.value.bias\n",
      "encoder.layer.5.attention.output.dense.weight\n",
      "encoder.layer.5.attention.output.dense.bias\n",
      "encoder.layer.5.attention.output.LayerNorm.weight\n",
      "encoder.layer.5.attention.output.LayerNorm.bias\n",
      "encoder.layer.5.intermediate.dense.weight\n",
      "encoder.layer.5.intermediate.dense.bias\n",
      "encoder.layer.5.output.dense.weight\n",
      "encoder.layer.5.output.dense.bias\n",
      "encoder.layer.5.output.LayerNorm.weight\n",
      "encoder.layer.5.output.LayerNorm.bias\n",
      "encoder.layer.6.attention.self.query.weight\n",
      "encoder.layer.6.attention.self.query.bias\n",
      "encoder.layer.6.attention.self.key.weight\n",
      "encoder.layer.6.attention.self.key.bias\n",
      "encoder.layer.6.attention.self.value.weight\n",
      "encoder.layer.6.attention.self.value.bias\n",
      "encoder.layer.6.attention.output.dense.weight\n",
      "encoder.layer.6.attention.output.dense.bias\n",
      "encoder.layer.6.attention.output.LayerNorm.weight\n",
      "encoder.layer.6.attention.output.LayerNorm.bias\n",
      "encoder.layer.6.intermediate.dense.weight\n",
      "encoder.layer.6.intermediate.dense.bias\n",
      "encoder.layer.6.output.dense.weight\n",
      "encoder.layer.6.output.dense.bias\n",
      "encoder.layer.6.output.LayerNorm.weight\n",
      "encoder.layer.6.output.LayerNorm.bias\n",
      "encoder.layer.7.attention.self.query.weight\n",
      "encoder.layer.7.attention.self.query.bias\n",
      "encoder.layer.7.attention.self.key.weight\n",
      "encoder.layer.7.attention.self.key.bias\n",
      "encoder.layer.7.attention.self.value.weight\n",
      "encoder.layer.7.attention.self.value.bias\n",
      "encoder.layer.7.attention.output.dense.weight\n",
      "encoder.layer.7.attention.output.dense.bias\n",
      "encoder.layer.7.attention.output.LayerNorm.weight\n",
      "encoder.layer.7.attention.output.LayerNorm.bias\n",
      "encoder.layer.7.intermediate.dense.weight\n",
      "encoder.layer.7.intermediate.dense.bias\n",
      "encoder.layer.7.output.dense.weight\n",
      "encoder.layer.7.output.dense.bias\n",
      "encoder.layer.7.output.LayerNorm.weight\n",
      "encoder.layer.7.output.LayerNorm.bias\n",
      "encoder.layer.8.attention.self.query.weight\n",
      "encoder.layer.8.attention.self.query.bias\n",
      "encoder.layer.8.attention.self.key.weight\n",
      "encoder.layer.8.attention.self.key.bias\n",
      "encoder.layer.8.attention.self.value.weight\n",
      "encoder.layer.8.attention.self.value.bias\n",
      "encoder.layer.8.attention.output.dense.weight\n",
      "encoder.layer.8.attention.output.dense.bias\n",
      "encoder.layer.8.attention.output.LayerNorm.weight\n",
      "encoder.layer.8.attention.output.LayerNorm.bias\n",
      "encoder.layer.8.intermediate.dense.weight\n",
      "encoder.layer.8.intermediate.dense.bias\n",
      "encoder.layer.8.output.dense.weight\n",
      "encoder.layer.8.output.dense.bias\n",
      "encoder.layer.8.output.LayerNorm.weight\n",
      "encoder.layer.8.output.LayerNorm.bias\n",
      "encoder.layer.9.attention.self.query.weight\n",
      "encoder.layer.9.attention.self.query.bias\n",
      "encoder.layer.9.attention.self.key.weight\n",
      "encoder.layer.9.attention.self.key.bias\n",
      "encoder.layer.9.attention.self.value.weight\n",
      "encoder.layer.9.attention.self.value.bias\n",
      "encoder.layer.9.attention.output.dense.weight\n",
      "encoder.layer.9.attention.output.dense.bias\n",
      "encoder.layer.9.attention.output.LayerNorm.weight\n",
      "encoder.layer.9.attention.output.LayerNorm.bias\n",
      "encoder.layer.9.intermediate.dense.weight\n",
      "encoder.layer.9.intermediate.dense.bias\n",
      "encoder.layer.9.output.dense.weight\n",
      "encoder.layer.9.output.dense.bias\n",
      "encoder.layer.9.output.LayerNorm.weight\n",
      "encoder.layer.9.output.LayerNorm.bias\n",
      "encoder.layer.10.attention.self.query.weight\n",
      "encoder.layer.10.attention.self.query.bias\n",
      "encoder.layer.10.attention.self.key.weight\n",
      "encoder.layer.10.attention.self.key.bias\n",
      "encoder.layer.10.attention.self.value.weight\n",
      "encoder.layer.10.attention.self.value.bias\n",
      "encoder.layer.10.attention.output.dense.weight\n",
      "encoder.layer.10.attention.output.dense.bias\n",
      "encoder.layer.10.attention.output.LayerNorm.weight\n",
      "encoder.layer.10.attention.output.LayerNorm.bias\n",
      "encoder.layer.10.intermediate.dense.weight\n",
      "encoder.layer.10.intermediate.dense.bias\n",
      "encoder.layer.10.output.dense.weight\n",
      "encoder.layer.10.output.dense.bias\n",
      "encoder.layer.10.output.LayerNorm.weight\n",
      "encoder.layer.10.output.LayerNorm.bias\n",
      "encoder.layer.11.attention.self.query.weight\n",
      "encoder.layer.11.attention.self.query.bias\n",
      "encoder.layer.11.attention.self.key.weight\n",
      "encoder.layer.11.attention.self.key.bias\n",
      "encoder.layer.11.attention.self.value.weight\n",
      "encoder.layer.11.attention.self.value.bias\n",
      "encoder.layer.11.attention.output.dense.weight\n",
      "encoder.layer.11.attention.output.dense.bias\n",
      "encoder.layer.11.attention.output.LayerNorm.weight\n",
      "encoder.layer.11.attention.output.LayerNorm.bias\n",
      "encoder.layer.11.intermediate.dense.weight\n",
      "encoder.layer.11.intermediate.dense.bias\n",
      "encoder.layer.11.output.dense.weight\n",
      "encoder.layer.11.output.dense.bias\n",
      "encoder.layer.11.output.LayerNorm.weight\n",
      "encoder.layer.11.output.LayerNorm.bias\n",
      "pooler.dense.weight\n",
      "pooler.dense.bias\n"
     ]
    }
   ],
   "source": [
    "for key in model_hf.state_dict().keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c2d9f70-b5fa-4a92-adac-d5604723f09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True, False, False],\n",
      "         [ True,  True,  True, False, False],\n",
      "         [ True,  True,  True, False, False],\n",
      "         [ True,  True,  True, False, False],\n",
      "         [ True,  True,  True, False, False]],\n",
      "\n",
      "        [[ True,  True, False, False, False],\n",
      "         [ True,  True, False, False, False],\n",
      "         [ True,  True, False, False, False],\n",
      "         [ True,  True, False, False, False],\n",
      "         [ True,  True, False, False, False]]])\n",
      "torch.Size([3, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3, 4, 5], [5,6,7,0, 0], [9,9,0,0,0]])\n",
    "# Create a mask where padding tokens are False \n",
    "# Create an extra dimension for each sequence\n",
    "# Repeat to mimic attention mask\n",
    "mask = (x > 0).unsqueeze(1).repeat(1, x.size(1),1)\n",
    "print(mask)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "53922f7d-9ac4-4a10-af02-ad8fbd1524d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.5500, -0.4970, -2.2028,  ...,  0.8643,  1.3536, -1.7909],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.4929,  1.2868, -1.4918,  ...,  0.8632, -1.5980,  0.0521],\n",
      "        ...,\n",
      "        [ 0.9052, -1.7088,  1.0249,  ...,  0.9630, -1.0406, -1.2196],\n",
      "        [-1.9594, -2.2697,  1.3669,  ...,  0.2475, -0.6283,  0.3957],\n",
      "        [-2.5360, -0.2193,  0.6976,  ...,  0.5029,  0.2769,  1.3981]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "t = nn.Embedding(512, 768, padding_idx = 1)\n",
    "print(t.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "83053e82-68d7-4a36-83d4-97965f72ec94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight\n",
      "embeddings.position_embeddings.weight\n",
      "embeddings.token_type_embeddings.weight\n",
      "embeddings.LayerNorm.weight\n",
      "embeddings.LayerNorm.bias\n",
      "encoder.layer.0.attention.query.weight\n",
      "encoder.layer.0.attention.query.bias\n",
      "encoder.layer.0.attention.key.weight\n",
      "encoder.layer.0.attention.key.bias\n",
      "encoder.layer.0.attention.value.weight\n",
      "encoder.layer.0.attention.value.bias\n",
      "encoder.layer.0.attention.output.weight\n",
      "encoder.layer.0.attention.output.bias\n",
      "encoder.layer.0.ln_1.weight\n",
      "encoder.layer.0.ln_1.bias\n",
      "encoder.layer.0.mlp.intermediate.weight\n",
      "encoder.layer.0.mlp.intermediate.bias\n",
      "encoder.layer.0.mlp.output.weight\n",
      "encoder.layer.0.mlp.output.bias\n",
      "encoder.layer.0.ln_2.weight\n",
      "encoder.layer.0.ln_2.bias\n",
      "encoder.layer.1.attention.query.weight\n",
      "encoder.layer.1.attention.query.bias\n",
      "encoder.layer.1.attention.key.weight\n",
      "encoder.layer.1.attention.key.bias\n",
      "encoder.layer.1.attention.value.weight\n",
      "encoder.layer.1.attention.value.bias\n",
      "encoder.layer.1.attention.output.weight\n",
      "encoder.layer.1.attention.output.bias\n",
      "encoder.layer.1.ln_1.weight\n",
      "encoder.layer.1.ln_1.bias\n",
      "encoder.layer.1.mlp.intermediate.weight\n",
      "encoder.layer.1.mlp.intermediate.bias\n",
      "encoder.layer.1.mlp.output.weight\n",
      "encoder.layer.1.mlp.output.bias\n",
      "encoder.layer.1.ln_2.weight\n",
      "encoder.layer.1.ln_2.bias\n",
      "encoder.layer.2.attention.query.weight\n",
      "encoder.layer.2.attention.query.bias\n",
      "encoder.layer.2.attention.key.weight\n",
      "encoder.layer.2.attention.key.bias\n",
      "encoder.layer.2.attention.value.weight\n",
      "encoder.layer.2.attention.value.bias\n",
      "encoder.layer.2.attention.output.weight\n",
      "encoder.layer.2.attention.output.bias\n",
      "encoder.layer.2.ln_1.weight\n",
      "encoder.layer.2.ln_1.bias\n",
      "encoder.layer.2.mlp.intermediate.weight\n",
      "encoder.layer.2.mlp.intermediate.bias\n",
      "encoder.layer.2.mlp.output.weight\n",
      "encoder.layer.2.mlp.output.bias\n",
      "encoder.layer.2.ln_2.weight\n",
      "encoder.layer.2.ln_2.bias\n",
      "encoder.layer.3.attention.query.weight\n",
      "encoder.layer.3.attention.query.bias\n",
      "encoder.layer.3.attention.key.weight\n",
      "encoder.layer.3.attention.key.bias\n",
      "encoder.layer.3.attention.value.weight\n",
      "encoder.layer.3.attention.value.bias\n",
      "encoder.layer.3.attention.output.weight\n",
      "encoder.layer.3.attention.output.bias\n",
      "encoder.layer.3.ln_1.weight\n",
      "encoder.layer.3.ln_1.bias\n",
      "encoder.layer.3.mlp.intermediate.weight\n",
      "encoder.layer.3.mlp.intermediate.bias\n",
      "encoder.layer.3.mlp.output.weight\n",
      "encoder.layer.3.mlp.output.bias\n",
      "encoder.layer.3.ln_2.weight\n",
      "encoder.layer.3.ln_2.bias\n",
      "encoder.layer.4.attention.query.weight\n",
      "encoder.layer.4.attention.query.bias\n",
      "encoder.layer.4.attention.key.weight\n",
      "encoder.layer.4.attention.key.bias\n",
      "encoder.layer.4.attention.value.weight\n",
      "encoder.layer.4.attention.value.bias\n",
      "encoder.layer.4.attention.output.weight\n",
      "encoder.layer.4.attention.output.bias\n",
      "encoder.layer.4.ln_1.weight\n",
      "encoder.layer.4.ln_1.bias\n",
      "encoder.layer.4.mlp.intermediate.weight\n",
      "encoder.layer.4.mlp.intermediate.bias\n",
      "encoder.layer.4.mlp.output.weight\n",
      "encoder.layer.4.mlp.output.bias\n",
      "encoder.layer.4.ln_2.weight\n",
      "encoder.layer.4.ln_2.bias\n",
      "encoder.layer.5.attention.query.weight\n",
      "encoder.layer.5.attention.query.bias\n",
      "encoder.layer.5.attention.key.weight\n",
      "encoder.layer.5.attention.key.bias\n",
      "encoder.layer.5.attention.value.weight\n",
      "encoder.layer.5.attention.value.bias\n",
      "encoder.layer.5.attention.output.weight\n",
      "encoder.layer.5.attention.output.bias\n",
      "encoder.layer.5.ln_1.weight\n",
      "encoder.layer.5.ln_1.bias\n",
      "encoder.layer.5.mlp.intermediate.weight\n",
      "encoder.layer.5.mlp.intermediate.bias\n",
      "encoder.layer.5.mlp.output.weight\n",
      "encoder.layer.5.mlp.output.bias\n",
      "encoder.layer.5.ln_2.weight\n",
      "encoder.layer.5.ln_2.bias\n",
      "encoder.layer.6.attention.query.weight\n",
      "encoder.layer.6.attention.query.bias\n",
      "encoder.layer.6.attention.key.weight\n",
      "encoder.layer.6.attention.key.bias\n",
      "encoder.layer.6.attention.value.weight\n",
      "encoder.layer.6.attention.value.bias\n",
      "encoder.layer.6.attention.output.weight\n",
      "encoder.layer.6.attention.output.bias\n",
      "encoder.layer.6.ln_1.weight\n",
      "encoder.layer.6.ln_1.bias\n",
      "encoder.layer.6.mlp.intermediate.weight\n",
      "encoder.layer.6.mlp.intermediate.bias\n",
      "encoder.layer.6.mlp.output.weight\n",
      "encoder.layer.6.mlp.output.bias\n",
      "encoder.layer.6.ln_2.weight\n",
      "encoder.layer.6.ln_2.bias\n",
      "encoder.layer.7.attention.query.weight\n",
      "encoder.layer.7.attention.query.bias\n",
      "encoder.layer.7.attention.key.weight\n",
      "encoder.layer.7.attention.key.bias\n",
      "encoder.layer.7.attention.value.weight\n",
      "encoder.layer.7.attention.value.bias\n",
      "encoder.layer.7.attention.output.weight\n",
      "encoder.layer.7.attention.output.bias\n",
      "encoder.layer.7.ln_1.weight\n",
      "encoder.layer.7.ln_1.bias\n",
      "encoder.layer.7.mlp.intermediate.weight\n",
      "encoder.layer.7.mlp.intermediate.bias\n",
      "encoder.layer.7.mlp.output.weight\n",
      "encoder.layer.7.mlp.output.bias\n",
      "encoder.layer.7.ln_2.weight\n",
      "encoder.layer.7.ln_2.bias\n",
      "encoder.layer.8.attention.query.weight\n",
      "encoder.layer.8.attention.query.bias\n",
      "encoder.layer.8.attention.key.weight\n",
      "encoder.layer.8.attention.key.bias\n",
      "encoder.layer.8.attention.value.weight\n",
      "encoder.layer.8.attention.value.bias\n",
      "encoder.layer.8.attention.output.weight\n",
      "encoder.layer.8.attention.output.bias\n",
      "encoder.layer.8.ln_1.weight\n",
      "encoder.layer.8.ln_1.bias\n",
      "encoder.layer.8.mlp.intermediate.weight\n",
      "encoder.layer.8.mlp.intermediate.bias\n",
      "encoder.layer.8.mlp.output.weight\n",
      "encoder.layer.8.mlp.output.bias\n",
      "encoder.layer.8.ln_2.weight\n",
      "encoder.layer.8.ln_2.bias\n",
      "encoder.layer.9.attention.query.weight\n",
      "encoder.layer.9.attention.query.bias\n",
      "encoder.layer.9.attention.key.weight\n",
      "encoder.layer.9.attention.key.bias\n",
      "encoder.layer.9.attention.value.weight\n",
      "encoder.layer.9.attention.value.bias\n",
      "encoder.layer.9.attention.output.weight\n",
      "encoder.layer.9.attention.output.bias\n",
      "encoder.layer.9.ln_1.weight\n",
      "encoder.layer.9.ln_1.bias\n",
      "encoder.layer.9.mlp.intermediate.weight\n",
      "encoder.layer.9.mlp.intermediate.bias\n",
      "encoder.layer.9.mlp.output.weight\n",
      "encoder.layer.9.mlp.output.bias\n",
      "encoder.layer.9.ln_2.weight\n",
      "encoder.layer.9.ln_2.bias\n",
      "encoder.layer.10.attention.query.weight\n",
      "encoder.layer.10.attention.query.bias\n",
      "encoder.layer.10.attention.key.weight\n",
      "encoder.layer.10.attention.key.bias\n",
      "encoder.layer.10.attention.value.weight\n",
      "encoder.layer.10.attention.value.bias\n",
      "encoder.layer.10.attention.output.weight\n",
      "encoder.layer.10.attention.output.bias\n",
      "encoder.layer.10.ln_1.weight\n",
      "encoder.layer.10.ln_1.bias\n",
      "encoder.layer.10.mlp.intermediate.weight\n",
      "encoder.layer.10.mlp.intermediate.bias\n",
      "encoder.layer.10.mlp.output.weight\n",
      "encoder.layer.10.mlp.output.bias\n",
      "encoder.layer.10.ln_2.weight\n",
      "encoder.layer.10.ln_2.bias\n",
      "encoder.layer.11.attention.query.weight\n",
      "encoder.layer.11.attention.query.bias\n",
      "encoder.layer.11.attention.key.weight\n",
      "encoder.layer.11.attention.key.bias\n",
      "encoder.layer.11.attention.value.weight\n",
      "encoder.layer.11.attention.value.bias\n",
      "encoder.layer.11.attention.output.weight\n",
      "encoder.layer.11.attention.output.bias\n",
      "encoder.layer.11.ln_1.weight\n",
      "encoder.layer.11.ln_1.bias\n",
      "encoder.layer.11.mlp.intermediate.weight\n",
      "encoder.layer.11.mlp.intermediate.bias\n",
      "encoder.layer.11.mlp.output.weight\n",
      "encoder.layer.11.mlp.output.bias\n",
      "encoder.layer.11.ln_2.weight\n",
      "encoder.layer.11.ln_2.bias\n",
      "pooler.weight\n",
      "pooler.bias\n"
     ]
    }
   ],
   "source": [
    "for key in model.state_dict().keys():\n",
    "    print(key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
