{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "ae360a05-9308-4a72-ae62-1c32dfadb3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "# No token_type_ids, just separate sequences with tokenizer.sep_token\n",
    "# So I guess RobertaTokenizer automatically adds <s> and </s> tokens to input\n",
    "# cls token is aparently <s> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69f480a-2660-461e-97a6-72b6b217f024",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, data_pair, tokenizer, seq_len=64):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        self.corpus_lines = len(data_pair)\n",
    "        self.lines = data_pair\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.corpus_lines\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "\n",
    "        # Step 1: get random sentence pair, either negative or positive (saved as is_next_label)\n",
    "        t1, t2, is_next_label = self.get_sent(item)\n",
    "\n",
    "        # Step 2: replace random words in sentence with mask / random words\n",
    "        t1_random, t1_label = self.random_word(t1)\n",
    "        t2_random, t2_label = self.random_word(t2)\n",
    "\n",
    "        # Step 3: Adding CLS and SEP tokens to the start and end of sentences\n",
    "         # Adding PAD token for labels\n",
    "        t1 = [self.tokenizer.vocab['[CLS]']] + t1_random + [self.tokenizer.vocab['[SEP]']]\n",
    "        t2 = t2_random + [self.tokenizer.vocab['[SEP]']]\n",
    "        t1_label = [self.tokenizer.vocab['[PAD]']] + t1_label + [self.tokenizer.vocab['[PAD]']]\n",
    "        t2_label = t2_label + [self.tokenizer.vocab['[PAD]']]\n",
    "\n",
    "        # Step 4: combine sentence 1 and 2 as one input\n",
    "        # adding PAD tokens to make the sentence same length as seq_len\n",
    "        segment_label = ([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:self.seq_len]\n",
    "        bert_input = (t1 + t2)[:self.seq_len]\n",
    "        bert_label = (t1_label + t2_label)[:self.seq_len]\n",
    "        padding = [self.tokenizer.vocab['[PAD]'] for _ in range(self.seq_len - len(bert_input))]\n",
    "        bert_input.extend(padding), bert_label.extend(padding), segment_label.extend(padding)\n",
    "\n",
    "        output = {\"bert_input\": bert_input,\n",
    "                  \"bert_label\": bert_label,\n",
    "                  \"segment_label\": segment_label,\n",
    "                  \"is_next\": is_next_label}\n",
    "\n",
    "        return {key: torch.tensor(value) for key, value in output.items()}\n",
    "\n",
    "    def random_word(self, sentence):\n",
    "        tokens = sentence.split()\n",
    "        output_label = []\n",
    "        output = []\n",
    "\n",
    "        # 15% of the tokens would be replaced\n",
    "        for i, token in enumerate(tokens):\n",
    "            prob = random.random()\n",
    "\n",
    "            # remove cls and sep token\n",
    "            token_id = self.tokenizer(token)['input_ids'][1:-1]\n",
    "\n",
    "            if prob < 0.15:\n",
    "                prob /= 0.15\n",
    "\n",
    "                # 80% chance change token to mask token\n",
    "                if prob < 0.8:\n",
    "                    for i in range(len(token_id)):\n",
    "                        output.append(self.tokenizer.vocab['[MASK]'])\n",
    "\n",
    "                # 10% chance change token to random token\n",
    "                elif prob < 0.9:\n",
    "                    for i in range(len(token_id)):\n",
    "                        output.append(random.randrange(len(self.tokenizer.vocab)))\n",
    "\n",
    "                # 10% chance change token to current token\n",
    "                else:\n",
    "                    output.append(token_id)\n",
    "\n",
    "                output_label.append(token_id)\n",
    "\n",
    "            else:\n",
    "                output.append(token_id)\n",
    "                for i in range(len(token_id)):\n",
    "                    output_label.append(0)\n",
    "\n",
    "        # flattening\n",
    "        output = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output]))\n",
    "        output_label = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output_label]))\n",
    "        assert len(output) == len(output_label)\n",
    "        return output, output_label\n",
    "\n",
    "    def get_sent(self, index):\n",
    "        '''return random sentence pair'''\n",
    "        t1, t2 = self.get_corpus_line(index)\n",
    "\n",
    "        # negative or positive pair, for next sentence prediction\n",
    "        if random.random() > 0.5:\n",
    "            return t1, t2, 1\n",
    "        else:\n",
    "            return t1, self.get_random_line(), 0\n",
    "\n",
    "    def get_corpus_line(self, item):\n",
    "        '''return sentence pair'''\n",
    "        return self.lines[item][0], self.lines[item][1]\n",
    "\n",
    "    def get_random_line(self):\n",
    "        '''return random single sentence'''\n",
    "        return self.lines[random.randrange(len(self.lines))][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "c8b01826-a21c-4ba1-972b-a0bd56bbac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->Roberta\n",
    "class RobertaSelfAttention(nn.Module):\n",
    "    def __init__(self, config, position_embedding_type=\"absolute\"):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.position_embedding_type = position_embedding_type\n",
    "\n",
    "        self.is_decoder = False\n",
    "\n",
    "    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "    \n",
    "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask is (precomputed for all layers in RobertaModel forward() function)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Copied from transformers.models.bert.modeling_bert.BertSdpaSelfAttention with Bert->Roberta\n",
    "class RobertaSdpaSelfAttention(RobertaSelfAttention):\n",
    "    def __init__(self, config, position_embedding_type=None):\n",
    "        super().__init__(config, position_embedding_type=position_embedding_type)\n",
    "        self.dropout_prob = config.attention_probs_dropout_prob\n",
    "        self.require_contiguous_qkv = False\n",
    "\n",
    "    # Adapted from RobertaSelfAttention\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        \n",
    "        bsz, tgt_len, _ = hidden_states.size()\n",
    "\n",
    "        query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
    "\n",
    "        # If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\n",
    "        # mask needs to be such that the encoder's padding tokens are not attended to.\n",
    "        is_cross_attention = encoder_hidden_states is not None\n",
    "\n",
    "        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n",
    "        attention_mask = encoder_attention_mask if is_cross_attention else attention_mask\n",
    "\n",
    "        # Check `seq_length` of `past_key_value` == `len(current_states)` to support prefix tuning\n",
    "        if is_cross_attention and past_key_value and past_key_value[0].shape[2] == current_states.shape[1]:\n",
    "            key_layer, value_layer = past_key_value\n",
    "        else:\n",
    "            key_layer = self.transpose_for_scores(self.key(current_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(current_states))\n",
    "            if past_key_value is not None and not is_cross_attention:\n",
    "                key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
    "                value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "            query_layer,\n",
    "            key_layer,\n",
    "            value_layer,\n",
    "            attn_mask=attention_mask,\n",
    "            dropout_p=self.dropout_prob if self.training else 0.0,\n",
    "            is_causal=is_causal,\n",
    "        )\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "        attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n",
    "\n",
    "        outputs = (attn_output,)\n",
    "        if self.is_decoder:\n",
    "            outputs = outputs + (past_key_value,)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373c54bd-ef7e-4579-9bb1-54d0a47107ee",
   "metadata": {},
   "source": [
    "## Roberta Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "3295324b-9cdf-4e2a-89ce-b85281992b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.intermediate = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.output = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.intermediate(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.output(x)\n",
    "        return x \n",
    "\n",
    "class FullSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.key = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.value = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        \n",
    "        self.output = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        \n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        B, T, C = x.size()\n",
    "        # (batch_size, max_len, hidden_size) \n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        q = q.view(B, T, self.num_attention_heads, C // self.num_attention_heads).transpose(1,2) # (B, num_head, T, head_size)\n",
    "        k = k.view(B, T, self.num_attention_heads, C // self.num_attention_heads).transpose(1,2)\n",
    "        v = v.view(B, T, self.num_attention_heads, C // self.num_attention_heads).transpose(1,2)\n",
    "\n",
    "        # Calculate attention scores \n",
    "        attn = (q @ k.transpose(2, 3)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = F.softmax(attn, dim = -1)\n",
    "\n",
    "        y = attn @ v # (B, nh, T, T) x (B, nh, T, hs) = (B, nh, T, hs)\n",
    "        y = y.transpose(1,2).contiguous().view(B,T,C) # Concat head outputs\n",
    "\n",
    "        # Project\n",
    "        y = self.output(y)        \n",
    "        return y\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.attention = FullSelfAttention(config)\n",
    "        self.ln_1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.mlp = MLP(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.hidden_size)\n",
    "        \n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = x + self.ln_1(self.attention(x, mask)) \n",
    "        x = x + self.ln_2(self.mlp(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class RobertaConfig:\n",
    "    vocab_size = 50265\n",
    "    hidden_size = 768 \n",
    "    num_hidden_layers = 12\n",
    "    num_attention_heads = 12\n",
    "    intermediate_size = 3072\n",
    "    max_position_embeddings = 512\n",
    "    \n",
    "    type_vocab_size = 1\n",
    "    pad_token_id = 1\n",
    "    bos_token_id = 0\n",
    "    eos_token_id = 2\n",
    "    \n",
    "\n",
    "\n",
    "class Roberta(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.embeddings = nn.ModuleDict(dict(\n",
    "            word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx = config.pad_token_id),\n",
    "            position_embeddings = nn.Embedding(config.max_position_embeddings + 2, config.hidden_size, padding_idx = config.pad_token_id),\n",
    "            token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size),\n",
    "            LayerNorm = nn.LayerNorm(config.hidden_size)\n",
    "        ))\n",
    "        \n",
    "        self.encoder = nn.ModuleDict(dict(\n",
    "            layer = nn.ModuleList([EncoderBlock(config) for _ in range(config.num_hidden_layers)])\n",
    "        ))\n",
    "        \n",
    "        self.lm_head = nn.ModuleDict(dict(\n",
    "            dense = nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            gelu = nn.GELU(),\n",
    "            LayerNorm = nn.LayerNorm(config.hidden_size),\n",
    "            decoder = nn.Linear(config.hidden_size, config.vocab_size),\n",
    "        ))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        # attention masking for padded token\n",
    "        # (batch_size, 1, seq_len, seq_len)\n",
    "        mask = (x >= 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1).to(x.device)\n",
    "\n",
    "        # Token embeddings\n",
    "        tok_emb = self.embeddings.word_embeddings(x)\n",
    "\n",
    "        # Positional embeddings \n",
    "        pos_mask = x.ne(self.config.pad_token_id).int()\n",
    "        indices = (((torch.cumsum(pos_mask,dim=1)).type_as(pos_mask))* pos_mask) + self.config.pad_token_id\n",
    "        pos_emb = self.embeddings.position_embeddings(indices)\n",
    "\n",
    "        # pos = torch.arange(0, T, dtype = torch.long, device = x.device)\n",
    "        # pos_emb = self.embeddings.position_embeddings(pos)\n",
    "        \n",
    "\n",
    "        \n",
    "        # Token Type embeddings\n",
    "        # typ = torch.zeros((1,T), dtype = torch.long, device = x.device)\n",
    "        # type_emb = self.embeddings.token_type_embeddings(typ)\n",
    "        \n",
    "\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        x = self.embeddings.LayerNorm(x)\n",
    "        \n",
    "        # Pass batch through transformer \n",
    "        for block in self.encoder.layer:\n",
    "            x = block(x, mask)\n",
    "\n",
    "        # Pass through prediction head\n",
    "        x = self.lm_head.dense(x)\n",
    "        x = self.lm_head.gelu(x)\n",
    "        x = self.lm_head.LayerNorm(x)\n",
    "        x = self.lm_head.decoder(x)\n",
    "\n",
    "        return x\n",
    "            \n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type = \"FacebookAI/roberta-base\"):\n",
    "        \"\"\" Loading pretrained Roberta weights from hugging face \"\"\"\n",
    "        print(\"loading weights for %s\" % model_type)\n",
    "\n",
    "        # Random init of model\n",
    "        config = RobertaConfig()\n",
    "        model = Roberta(config)\n",
    "        \n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "\n",
    "        # Init a Roberta from hugging face \n",
    "        model_hf = RobertaForMaskedLM.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "        sd_hf_keys = [k for k in sd_hf.keys() if not k.endswith('lm_head.bias')]\n",
    "        # Copy over weights. State Dicts are currently in same order, so I can just blind copy \n",
    "        for keys in zip(sd_keys, sd_hf_keys):\n",
    "            # print(sd[keys[0]].shape)\n",
    "            # print(sd_hf[keys[1]].shape)\n",
    "            \n",
    "            assert(sd[keys[0]].shape == sd_hf[keys[1]].shape)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                sd[keys[0]].copy_(sd_hf[keys[1]])\n",
    "\n",
    "        return model\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3441be52-58e1-4d2b-9835-280aaa653bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "422bf265-85ba-4edb-a147-399bae279366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights for FacebookAI/roberta-base\n",
      "tensor([[    0,   133,   812,     9, 50264,    16,  2201,     2]],\n",
      "       device='mps:0')\n",
      ",\n"
     ]
    }
   ],
   "source": [
    "model = Roberta.from_pretrained().to(device)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "x = \"The capital of <mask> is Paris\"\n",
    "x = torch.tensor(tokenizer.encode(x)).unsqueeze(0).to(device)\n",
    "print(x)\n",
    "with torch.no_grad():\n",
    "    logits = model(x)\n",
    "\n",
    "mask_token_index = (x == tokenizer.mask_token_id)[0].nonzero(as_tuple = True)[0]\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "print(tokenizer.decode(predicted_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "85783036-7745-4da3-abc6-19515c6fb2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",,,,,,,,\n",
      "tensor([ 0.6646, -5.2787,  9.3434,  ..., -3.4416, -4.1338,  4.4339],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "o = logits[0].argmax(axis=-1)\n",
    "print(tokenizer.decode(o))\n",
    "print(logits[0, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "feffea3f-a86d-4659-9781-8f87fc36fc1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RobertaLMHeadModel' from 'transformers' (/opt/anaconda3/envs/pytorch-gpu/lib/python3.13/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[346], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RobertaLMHeadModel\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'RobertaLMHeadModel' from 'transformers' (/opt/anaconda3/envs/pytorch-gpu/lib/python3.13/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaLMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "20a55635-208e-460b-874e-25a02e3dea02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   133,   812,     9,  1470,    16, 50264,     4,     1,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "torch.Size([1, 10, 50265])\n",
      " Paris\n",
      "MaskedLMOutput(loss=tensor(0.0990, grad_fn=<NllLossBackward0>), logits=tensor([[[34.2751, -3.7783, 18.3324,  ...,  2.7908,  5.3139, 11.8587],\n",
      "         [ 8.6809, -2.8665, 18.9842,  ...,  2.8312,  4.0936,  9.4246],\n",
      "         [-3.3497, -4.3248,  8.5588,  ..., -1.9859, -2.6968,  0.3392],\n",
      "         ...,\n",
      "         [21.2609, -4.2944, 19.6318,  ...,  0.9557,  3.3131,  8.0253],\n",
      "         [10.4854, -4.2014, 28.6527,  ..., -1.6521, -3.9379,  8.8991],\n",
      "         [11.1121, -3.5715, 31.1623,  ...,  1.5217, -0.4953,  9.6180]]],\n",
      "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, RobertaForMaskedLM, RobertaForCasualLM\n",
    "import torch\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "model = RobertaForMaskedLM.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "inputs = tokenizer(\"The capital of France is <mask>.<pad>\", return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "print(logits.shape)\n",
    "# retrieve index of <mask>\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "print(tokenizer.decode(predicted_token_id))\n",
    "\n",
    "labels = tokenizer(\"The capital of France is Paris.<pad>\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "# mask labels of non-<mask> tokens\n",
    "labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
    "\n",
    "outputs = model(**inputs, labels=labels)\n",
    "print(outputs)\n",
    "round(outputs.loss.item(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "7dbf0ae7-a061-44c2-9c42-2cb0d8bdaa49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights for FacebookAI/roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "model = Roberta.from_pretrained().to(device)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "x = \"The capital of France is <mask>.<pad>\"\n",
    "x = torch.tensor(tokenizer.encode(x)).unsqueeze(0).to(device)\n",
    "\n",
    "logits = model.forward(x)\n",
    "print(logits.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "846674b5-5102-4c93-b1ce-c219114a78ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2201)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0,6].argmax(axis=-1)\n",
    "print(tokenizer.decoder(predicted_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "28b4af17-63d0-4466-9d3f-e289f3179089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n",
    "    \"\"\"\n",
    "    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n",
    "    are ignored. This is modified from fairseq's `utils.make_positions`.\n",
    "\n",
    "    Args:\n",
    "        x: torch.Tensor x:\n",
    "\n",
    "    Returns: torch.Tensor\n",
    "    \"\"\"\n",
    "    # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n",
    "    mask = input_ids.ne(padding_idx).int()\n",
    "    incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n",
    "    return incremental_indices.long() + padding_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "8655e9fc-a709-49b8-8ff6-db39a89c025b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 0, 4, 5, 6],\n",
      "        [1, 2, 3, 0, 4, 5, 6]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[0,2,3,4,1,1,1],[0,5,2,4,9,1,1]])\n",
    "mask = x.ne(4).int()\n",
    "\n",
    "ind = ((torch.cumsum(mask,dim=1)).type_as(mask))* mask \n",
    "print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "cefa99e0-1f05-409e-8a0b-d2e43739d078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights for FacebookAI/roberta-base\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[354], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m RobertaConfig()\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mRoberta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# x = torch.tensor([[1, 2, 3, 4, 5], [5,6,7,0, 0], [9,9,0,0,0]])\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# print(model.forward(x))\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[344], line 171\u001b[0m, in \u001b[0;36mRoberta.from_pretrained\u001b[0;34m(cls, model_type)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# Copy over weights. State Dicts are currently in same order, so I can just blind copy \u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keys \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sd_keys, sd_hf_keys):\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# print(sd[keys[0]].shape)\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# print(sd_hf[keys[1]].shape)\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m(sd[keys[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m sd_hf[keys[\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    174\u001b[0m         sd[keys[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39mcopy_(sd_hf[keys[\u001b[38;5;241m1\u001b[39m]])\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config = RobertaConfig()\n",
    "model = Roberta.from_pretrained()\n",
    "\n",
    "# x = torch.tensor([[1, 2, 3, 4, 5], [5,6,7,0, 0], [9,9,0,0,0]])\n",
    "# print(model.forward(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "11438ad2-3e42-4ec6-a34b-3846bf341850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "model_hf = RobertaModel.from_pretrained(\"FacebookAI/roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b288df6a-109c-4038-91b6-8c1994ec44d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 133, 812, 9, 1470, 16, 50264, 4, 1, 1, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer('The capital of France is <mask>.<pad><pad>')\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "751ca591-3fc2-4c04-bf71-7e976e55455f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForMaskedLM(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): RobertaLMHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model_hf = RobertaForMaskedLM.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "from transformers import AutoTokenizer, RobertaForMaskedLM, RobertaForCausalLM, RobertaForSequenceClassification, RobertaForQuestionAnswering\n",
    "\n",
    "model_hf = RobertaForMaskedLM.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "print(model_hf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "573ab34f-615e-4ad6-94e7-9b63a421b6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.embeddings.word_embeddings.weight\n",
      "roberta.embeddings.position_embeddings.weight\n",
      "roberta.embeddings.token_type_embeddings.weight\n",
      "roberta.embeddings.LayerNorm.weight\n",
      "roberta.embeddings.LayerNorm.bias\n",
      "roberta.encoder.layer.0.attention.self.query.weight\n",
      "roberta.encoder.layer.0.attention.self.query.bias\n",
      "roberta.encoder.layer.0.attention.self.key.weight\n",
      "roberta.encoder.layer.0.attention.self.key.bias\n",
      "roberta.encoder.layer.0.attention.self.value.weight\n",
      "roberta.encoder.layer.0.attention.self.value.bias\n",
      "roberta.encoder.layer.0.attention.output.dense.weight\n",
      "roberta.encoder.layer.0.attention.output.dense.bias\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.0.intermediate.dense.weight\n",
      "roberta.encoder.layer.0.intermediate.dense.bias\n",
      "roberta.encoder.layer.0.output.dense.weight\n",
      "roberta.encoder.layer.0.output.dense.bias\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias\n",
      "roberta.encoder.layer.1.attention.self.query.weight\n",
      "roberta.encoder.layer.1.attention.self.query.bias\n",
      "roberta.encoder.layer.1.attention.self.key.weight\n",
      "roberta.encoder.layer.1.attention.self.key.bias\n",
      "roberta.encoder.layer.1.attention.self.value.weight\n",
      "roberta.encoder.layer.1.attention.self.value.bias\n",
      "roberta.encoder.layer.1.attention.output.dense.weight\n",
      "roberta.encoder.layer.1.attention.output.dense.bias\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.1.intermediate.dense.weight\n",
      "roberta.encoder.layer.1.intermediate.dense.bias\n",
      "roberta.encoder.layer.1.output.dense.weight\n",
      "roberta.encoder.layer.1.output.dense.bias\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias\n",
      "roberta.encoder.layer.2.attention.self.query.weight\n",
      "roberta.encoder.layer.2.attention.self.query.bias\n",
      "roberta.encoder.layer.2.attention.self.key.weight\n",
      "roberta.encoder.layer.2.attention.self.key.bias\n",
      "roberta.encoder.layer.2.attention.self.value.weight\n",
      "roberta.encoder.layer.2.attention.self.value.bias\n",
      "roberta.encoder.layer.2.attention.output.dense.weight\n",
      "roberta.encoder.layer.2.attention.output.dense.bias\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.2.intermediate.dense.weight\n",
      "roberta.encoder.layer.2.intermediate.dense.bias\n",
      "roberta.encoder.layer.2.output.dense.weight\n",
      "roberta.encoder.layer.2.output.dense.bias\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.attention.self.query.weight\n",
      "roberta.encoder.layer.3.attention.self.query.bias\n",
      "roberta.encoder.layer.3.attention.self.key.weight\n",
      "roberta.encoder.layer.3.attention.self.key.bias\n",
      "roberta.encoder.layer.3.attention.self.value.weight\n",
      "roberta.encoder.layer.3.attention.self.value.bias\n",
      "roberta.encoder.layer.3.attention.output.dense.weight\n",
      "roberta.encoder.layer.3.attention.output.dense.bias\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.intermediate.dense.weight\n",
      "roberta.encoder.layer.3.intermediate.dense.bias\n",
      "roberta.encoder.layer.3.output.dense.weight\n",
      "roberta.encoder.layer.3.output.dense.bias\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.attention.self.query.weight\n",
      "roberta.encoder.layer.4.attention.self.query.bias\n",
      "roberta.encoder.layer.4.attention.self.key.weight\n",
      "roberta.encoder.layer.4.attention.self.key.bias\n",
      "roberta.encoder.layer.4.attention.self.value.weight\n",
      "roberta.encoder.layer.4.attention.self.value.bias\n",
      "roberta.encoder.layer.4.attention.output.dense.weight\n",
      "roberta.encoder.layer.4.attention.output.dense.bias\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.intermediate.dense.weight\n",
      "roberta.encoder.layer.4.intermediate.dense.bias\n",
      "roberta.encoder.layer.4.output.dense.weight\n",
      "roberta.encoder.layer.4.output.dense.bias\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.attention.self.query.weight\n",
      "roberta.encoder.layer.5.attention.self.query.bias\n",
      "roberta.encoder.layer.5.attention.self.key.weight\n",
      "roberta.encoder.layer.5.attention.self.key.bias\n",
      "roberta.encoder.layer.5.attention.self.value.weight\n",
      "roberta.encoder.layer.5.attention.self.value.bias\n",
      "roberta.encoder.layer.5.attention.output.dense.weight\n",
      "roberta.encoder.layer.5.attention.output.dense.bias\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.intermediate.dense.weight\n",
      "roberta.encoder.layer.5.intermediate.dense.bias\n",
      "roberta.encoder.layer.5.output.dense.weight\n",
      "roberta.encoder.layer.5.output.dense.bias\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.attention.self.query.weight\n",
      "roberta.encoder.layer.6.attention.self.query.bias\n",
      "roberta.encoder.layer.6.attention.self.key.weight\n",
      "roberta.encoder.layer.6.attention.self.key.bias\n",
      "roberta.encoder.layer.6.attention.self.value.weight\n",
      "roberta.encoder.layer.6.attention.self.value.bias\n",
      "roberta.encoder.layer.6.attention.output.dense.weight\n",
      "roberta.encoder.layer.6.attention.output.dense.bias\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.intermediate.dense.weight\n",
      "roberta.encoder.layer.6.intermediate.dense.bias\n",
      "roberta.encoder.layer.6.output.dense.weight\n",
      "roberta.encoder.layer.6.output.dense.bias\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.attention.self.query.weight\n",
      "roberta.encoder.layer.7.attention.self.query.bias\n",
      "roberta.encoder.layer.7.attention.self.key.weight\n",
      "roberta.encoder.layer.7.attention.self.key.bias\n",
      "roberta.encoder.layer.7.attention.self.value.weight\n",
      "roberta.encoder.layer.7.attention.self.value.bias\n",
      "roberta.encoder.layer.7.attention.output.dense.weight\n",
      "roberta.encoder.layer.7.attention.output.dense.bias\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.intermediate.dense.weight\n",
      "roberta.encoder.layer.7.intermediate.dense.bias\n",
      "roberta.encoder.layer.7.output.dense.weight\n",
      "roberta.encoder.layer.7.output.dense.bias\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.attention.self.query.weight\n",
      "roberta.encoder.layer.8.attention.self.query.bias\n",
      "roberta.encoder.layer.8.attention.self.key.weight\n",
      "roberta.encoder.layer.8.attention.self.key.bias\n",
      "roberta.encoder.layer.8.attention.self.value.weight\n",
      "roberta.encoder.layer.8.attention.self.value.bias\n",
      "roberta.encoder.layer.8.attention.output.dense.weight\n",
      "roberta.encoder.layer.8.attention.output.dense.bias\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.intermediate.dense.weight\n",
      "roberta.encoder.layer.8.intermediate.dense.bias\n",
      "roberta.encoder.layer.8.output.dense.weight\n",
      "roberta.encoder.layer.8.output.dense.bias\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.attention.self.query.weight\n",
      "roberta.encoder.layer.9.attention.self.query.bias\n",
      "roberta.encoder.layer.9.attention.self.key.weight\n",
      "roberta.encoder.layer.9.attention.self.key.bias\n",
      "roberta.encoder.layer.9.attention.self.value.weight\n",
      "roberta.encoder.layer.9.attention.self.value.bias\n",
      "roberta.encoder.layer.9.attention.output.dense.weight\n",
      "roberta.encoder.layer.9.attention.output.dense.bias\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.intermediate.dense.weight\n",
      "roberta.encoder.layer.9.intermediate.dense.bias\n",
      "roberta.encoder.layer.9.output.dense.weight\n",
      "roberta.encoder.layer.9.output.dense.bias\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.attention.self.query.weight\n",
      "roberta.encoder.layer.10.attention.self.query.bias\n",
      "roberta.encoder.layer.10.attention.self.key.weight\n",
      "roberta.encoder.layer.10.attention.self.key.bias\n",
      "roberta.encoder.layer.10.attention.self.value.weight\n",
      "roberta.encoder.layer.10.attention.self.value.bias\n",
      "roberta.encoder.layer.10.attention.output.dense.weight\n",
      "roberta.encoder.layer.10.attention.output.dense.bias\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.intermediate.dense.weight\n",
      "roberta.encoder.layer.10.intermediate.dense.bias\n",
      "roberta.encoder.layer.10.output.dense.weight\n",
      "roberta.encoder.layer.10.output.dense.bias\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.attention.self.query.weight\n",
      "roberta.encoder.layer.11.attention.self.query.bias\n",
      "roberta.encoder.layer.11.attention.self.key.weight\n",
      "roberta.encoder.layer.11.attention.self.key.bias\n",
      "roberta.encoder.layer.11.attention.self.value.weight\n",
      "roberta.encoder.layer.11.attention.self.value.bias\n",
      "roberta.encoder.layer.11.attention.output.dense.weight\n",
      "roberta.encoder.layer.11.attention.output.dense.bias\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.intermediate.dense.weight\n",
      "roberta.encoder.layer.11.intermediate.dense.bias\n",
      "roberta.encoder.layer.11.output.dense.weight\n",
      "roberta.encoder.layer.11.output.dense.bias\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias\n",
      "lm_head.bias\n",
      "lm_head.dense.weight\n",
      "lm_head.dense.bias\n",
      "lm_head.layer_norm.weight\n",
      "lm_head.layer_norm.bias\n",
      "lm_head.decoder.weight\n",
      "lm_head.decoder.bias\n"
     ]
    }
   ],
   "source": [
    "for k in model_hf.state_dict().keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "85c281e7-ee83-43c3-bcfa-1bfefa1caea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea1577df-bbbb-46b8-816f-c2c330c42221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight\n",
      "embeddings.position_embeddings.weight\n",
      "embeddings.token_type_embeddings.weight\n",
      "embeddings.LayerNorm.weight\n",
      "embeddings.LayerNorm.bias\n",
      "encoder.layer.0.attention.self.query.weight\n",
      "encoder.layer.0.attention.self.query.bias\n",
      "encoder.layer.0.attention.self.key.weight\n",
      "encoder.layer.0.attention.self.key.bias\n",
      "encoder.layer.0.attention.self.value.weight\n",
      "encoder.layer.0.attention.self.value.bias\n",
      "encoder.layer.0.attention.output.dense.weight\n",
      "encoder.layer.0.attention.output.dense.bias\n",
      "encoder.layer.0.attention.output.LayerNorm.weight\n",
      "encoder.layer.0.attention.output.LayerNorm.bias\n",
      "encoder.layer.0.intermediate.dense.weight\n",
      "encoder.layer.0.intermediate.dense.bias\n",
      "encoder.layer.0.output.dense.weight\n",
      "encoder.layer.0.output.dense.bias\n",
      "encoder.layer.0.output.LayerNorm.weight\n",
      "encoder.layer.0.output.LayerNorm.bias\n",
      "encoder.layer.1.attention.self.query.weight\n",
      "encoder.layer.1.attention.self.query.bias\n",
      "encoder.layer.1.attention.self.key.weight\n",
      "encoder.layer.1.attention.self.key.bias\n",
      "encoder.layer.1.attention.self.value.weight\n",
      "encoder.layer.1.attention.self.value.bias\n",
      "encoder.layer.1.attention.output.dense.weight\n",
      "encoder.layer.1.attention.output.dense.bias\n",
      "encoder.layer.1.attention.output.LayerNorm.weight\n",
      "encoder.layer.1.attention.output.LayerNorm.bias\n",
      "encoder.layer.1.intermediate.dense.weight\n",
      "encoder.layer.1.intermediate.dense.bias\n",
      "encoder.layer.1.output.dense.weight\n",
      "encoder.layer.1.output.dense.bias\n",
      "encoder.layer.1.output.LayerNorm.weight\n",
      "encoder.layer.1.output.LayerNorm.bias\n",
      "encoder.layer.2.attention.self.query.weight\n",
      "encoder.layer.2.attention.self.query.bias\n",
      "encoder.layer.2.attention.self.key.weight\n",
      "encoder.layer.2.attention.self.key.bias\n",
      "encoder.layer.2.attention.self.value.weight\n",
      "encoder.layer.2.attention.self.value.bias\n",
      "encoder.layer.2.attention.output.dense.weight\n",
      "encoder.layer.2.attention.output.dense.bias\n",
      "encoder.layer.2.attention.output.LayerNorm.weight\n",
      "encoder.layer.2.attention.output.LayerNorm.bias\n",
      "encoder.layer.2.intermediate.dense.weight\n",
      "encoder.layer.2.intermediate.dense.bias\n",
      "encoder.layer.2.output.dense.weight\n",
      "encoder.layer.2.output.dense.bias\n",
      "encoder.layer.2.output.LayerNorm.weight\n",
      "encoder.layer.2.output.LayerNorm.bias\n",
      "encoder.layer.3.attention.self.query.weight\n",
      "encoder.layer.3.attention.self.query.bias\n",
      "encoder.layer.3.attention.self.key.weight\n",
      "encoder.layer.3.attention.self.key.bias\n",
      "encoder.layer.3.attention.self.value.weight\n",
      "encoder.layer.3.attention.self.value.bias\n",
      "encoder.layer.3.attention.output.dense.weight\n",
      "encoder.layer.3.attention.output.dense.bias\n",
      "encoder.layer.3.attention.output.LayerNorm.weight\n",
      "encoder.layer.3.attention.output.LayerNorm.bias\n",
      "encoder.layer.3.intermediate.dense.weight\n",
      "encoder.layer.3.intermediate.dense.bias\n",
      "encoder.layer.3.output.dense.weight\n",
      "encoder.layer.3.output.dense.bias\n",
      "encoder.layer.3.output.LayerNorm.weight\n",
      "encoder.layer.3.output.LayerNorm.bias\n",
      "encoder.layer.4.attention.self.query.weight\n",
      "encoder.layer.4.attention.self.query.bias\n",
      "encoder.layer.4.attention.self.key.weight\n",
      "encoder.layer.4.attention.self.key.bias\n",
      "encoder.layer.4.attention.self.value.weight\n",
      "encoder.layer.4.attention.self.value.bias\n",
      "encoder.layer.4.attention.output.dense.weight\n",
      "encoder.layer.4.attention.output.dense.bias\n",
      "encoder.layer.4.attention.output.LayerNorm.weight\n",
      "encoder.layer.4.attention.output.LayerNorm.bias\n",
      "encoder.layer.4.intermediate.dense.weight\n",
      "encoder.layer.4.intermediate.dense.bias\n",
      "encoder.layer.4.output.dense.weight\n",
      "encoder.layer.4.output.dense.bias\n",
      "encoder.layer.4.output.LayerNorm.weight\n",
      "encoder.layer.4.output.LayerNorm.bias\n",
      "encoder.layer.5.attention.self.query.weight\n",
      "encoder.layer.5.attention.self.query.bias\n",
      "encoder.layer.5.attention.self.key.weight\n",
      "encoder.layer.5.attention.self.key.bias\n",
      "encoder.layer.5.attention.self.value.weight\n",
      "encoder.layer.5.attention.self.value.bias\n",
      "encoder.layer.5.attention.output.dense.weight\n",
      "encoder.layer.5.attention.output.dense.bias\n",
      "encoder.layer.5.attention.output.LayerNorm.weight\n",
      "encoder.layer.5.attention.output.LayerNorm.bias\n",
      "encoder.layer.5.intermediate.dense.weight\n",
      "encoder.layer.5.intermediate.dense.bias\n",
      "encoder.layer.5.output.dense.weight\n",
      "encoder.layer.5.output.dense.bias\n",
      "encoder.layer.5.output.LayerNorm.weight\n",
      "encoder.layer.5.output.LayerNorm.bias\n",
      "encoder.layer.6.attention.self.query.weight\n",
      "encoder.layer.6.attention.self.query.bias\n",
      "encoder.layer.6.attention.self.key.weight\n",
      "encoder.layer.6.attention.self.key.bias\n",
      "encoder.layer.6.attention.self.value.weight\n",
      "encoder.layer.6.attention.self.value.bias\n",
      "encoder.layer.6.attention.output.dense.weight\n",
      "encoder.layer.6.attention.output.dense.bias\n",
      "encoder.layer.6.attention.output.LayerNorm.weight\n",
      "encoder.layer.6.attention.output.LayerNorm.bias\n",
      "encoder.layer.6.intermediate.dense.weight\n",
      "encoder.layer.6.intermediate.dense.bias\n",
      "encoder.layer.6.output.dense.weight\n",
      "encoder.layer.6.output.dense.bias\n",
      "encoder.layer.6.output.LayerNorm.weight\n",
      "encoder.layer.6.output.LayerNorm.bias\n",
      "encoder.layer.7.attention.self.query.weight\n",
      "encoder.layer.7.attention.self.query.bias\n",
      "encoder.layer.7.attention.self.key.weight\n",
      "encoder.layer.7.attention.self.key.bias\n",
      "encoder.layer.7.attention.self.value.weight\n",
      "encoder.layer.7.attention.self.value.bias\n",
      "encoder.layer.7.attention.output.dense.weight\n",
      "encoder.layer.7.attention.output.dense.bias\n",
      "encoder.layer.7.attention.output.LayerNorm.weight\n",
      "encoder.layer.7.attention.output.LayerNorm.bias\n",
      "encoder.layer.7.intermediate.dense.weight\n",
      "encoder.layer.7.intermediate.dense.bias\n",
      "encoder.layer.7.output.dense.weight\n",
      "encoder.layer.7.output.dense.bias\n",
      "encoder.layer.7.output.LayerNorm.weight\n",
      "encoder.layer.7.output.LayerNorm.bias\n",
      "encoder.layer.8.attention.self.query.weight\n",
      "encoder.layer.8.attention.self.query.bias\n",
      "encoder.layer.8.attention.self.key.weight\n",
      "encoder.layer.8.attention.self.key.bias\n",
      "encoder.layer.8.attention.self.value.weight\n",
      "encoder.layer.8.attention.self.value.bias\n",
      "encoder.layer.8.attention.output.dense.weight\n",
      "encoder.layer.8.attention.output.dense.bias\n",
      "encoder.layer.8.attention.output.LayerNorm.weight\n",
      "encoder.layer.8.attention.output.LayerNorm.bias\n",
      "encoder.layer.8.intermediate.dense.weight\n",
      "encoder.layer.8.intermediate.dense.bias\n",
      "encoder.layer.8.output.dense.weight\n",
      "encoder.layer.8.output.dense.bias\n",
      "encoder.layer.8.output.LayerNorm.weight\n",
      "encoder.layer.8.output.LayerNorm.bias\n",
      "encoder.layer.9.attention.self.query.weight\n",
      "encoder.layer.9.attention.self.query.bias\n",
      "encoder.layer.9.attention.self.key.weight\n",
      "encoder.layer.9.attention.self.key.bias\n",
      "encoder.layer.9.attention.self.value.weight\n",
      "encoder.layer.9.attention.self.value.bias\n",
      "encoder.layer.9.attention.output.dense.weight\n",
      "encoder.layer.9.attention.output.dense.bias\n",
      "encoder.layer.9.attention.output.LayerNorm.weight\n",
      "encoder.layer.9.attention.output.LayerNorm.bias\n",
      "encoder.layer.9.intermediate.dense.weight\n",
      "encoder.layer.9.intermediate.dense.bias\n",
      "encoder.layer.9.output.dense.weight\n",
      "encoder.layer.9.output.dense.bias\n",
      "encoder.layer.9.output.LayerNorm.weight\n",
      "encoder.layer.9.output.LayerNorm.bias\n",
      "encoder.layer.10.attention.self.query.weight\n",
      "encoder.layer.10.attention.self.query.bias\n",
      "encoder.layer.10.attention.self.key.weight\n",
      "encoder.layer.10.attention.self.key.bias\n",
      "encoder.layer.10.attention.self.value.weight\n",
      "encoder.layer.10.attention.self.value.bias\n",
      "encoder.layer.10.attention.output.dense.weight\n",
      "encoder.layer.10.attention.output.dense.bias\n",
      "encoder.layer.10.attention.output.LayerNorm.weight\n",
      "encoder.layer.10.attention.output.LayerNorm.bias\n",
      "encoder.layer.10.intermediate.dense.weight\n",
      "encoder.layer.10.intermediate.dense.bias\n",
      "encoder.layer.10.output.dense.weight\n",
      "encoder.layer.10.output.dense.bias\n",
      "encoder.layer.10.output.LayerNorm.weight\n",
      "encoder.layer.10.output.LayerNorm.bias\n",
      "encoder.layer.11.attention.self.query.weight\n",
      "encoder.layer.11.attention.self.query.bias\n",
      "encoder.layer.11.attention.self.key.weight\n",
      "encoder.layer.11.attention.self.key.bias\n",
      "encoder.layer.11.attention.self.value.weight\n",
      "encoder.layer.11.attention.self.value.bias\n",
      "encoder.layer.11.attention.output.dense.weight\n",
      "encoder.layer.11.attention.output.dense.bias\n",
      "encoder.layer.11.attention.output.LayerNorm.weight\n",
      "encoder.layer.11.attention.output.LayerNorm.bias\n",
      "encoder.layer.11.intermediate.dense.weight\n",
      "encoder.layer.11.intermediate.dense.bias\n",
      "encoder.layer.11.output.dense.weight\n",
      "encoder.layer.11.output.dense.bias\n",
      "encoder.layer.11.output.LayerNorm.weight\n",
      "encoder.layer.11.output.LayerNorm.bias\n",
      "pooler.dense.weight\n",
      "pooler.dense.bias\n"
     ]
    }
   ],
   "source": [
    "for key in model_hf.state_dict().keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c2d9f70-b5fa-4a92-adac-d5604723f09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True, False, False],\n",
      "         [ True,  True,  True, False, False],\n",
      "         [ True,  True,  True, False, False],\n",
      "         [ True,  True,  True, False, False],\n",
      "         [ True,  True,  True, False, False]],\n",
      "\n",
      "        [[ True,  True, False, False, False],\n",
      "         [ True,  True, False, False, False],\n",
      "         [ True,  True, False, False, False],\n",
      "         [ True,  True, False, False, False],\n",
      "         [ True,  True, False, False, False]]])\n",
      "torch.Size([3, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3, 4, 5], [5,6,7,0, 0], [9,9,0,0,0]])\n",
    "# Create a mask where padding tokens are False \n",
    "# Create an extra dimension for each sequence\n",
    "# Repeat to mimic attention mask\n",
    "mask = (x > 0).unsqueeze(1).repeat(1, x.size(1),1)\n",
    "print(mask)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "53922f7d-9ac4-4a10-af02-ad8fbd1524d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.5500, -0.4970, -2.2028,  ...,  0.8643,  1.3536, -1.7909],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.4929,  1.2868, -1.4918,  ...,  0.8632, -1.5980,  0.0521],\n",
      "        ...,\n",
      "        [ 0.9052, -1.7088,  1.0249,  ...,  0.9630, -1.0406, -1.2196],\n",
      "        [-1.9594, -2.2697,  1.3669,  ...,  0.2475, -0.6283,  0.3957],\n",
      "        [-2.5360, -0.2193,  0.6976,  ...,  0.5029,  0.2769,  1.3981]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "t = nn.Embedding(512, 768, padding_idx = 1)\n",
    "print(t.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "83053e82-68d7-4a36-83d4-97965f72ec94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight\n",
      "embeddings.position_embeddings.weight\n",
      "embeddings.token_type_embeddings.weight\n",
      "embeddings.LayerNorm.weight\n",
      "embeddings.LayerNorm.bias\n",
      "encoder.layer.0.attention.query.weight\n",
      "encoder.layer.0.attention.query.bias\n",
      "encoder.layer.0.attention.key.weight\n",
      "encoder.layer.0.attention.key.bias\n",
      "encoder.layer.0.attention.value.weight\n",
      "encoder.layer.0.attention.value.bias\n",
      "encoder.layer.0.attention.output.weight\n",
      "encoder.layer.0.attention.output.bias\n",
      "encoder.layer.0.ln_1.weight\n",
      "encoder.layer.0.ln_1.bias\n",
      "encoder.layer.0.mlp.intermediate.weight\n",
      "encoder.layer.0.mlp.intermediate.bias\n",
      "encoder.layer.0.mlp.output.weight\n",
      "encoder.layer.0.mlp.output.bias\n",
      "encoder.layer.0.ln_2.weight\n",
      "encoder.layer.0.ln_2.bias\n",
      "encoder.layer.1.attention.query.weight\n",
      "encoder.layer.1.attention.query.bias\n",
      "encoder.layer.1.attention.key.weight\n",
      "encoder.layer.1.attention.key.bias\n",
      "encoder.layer.1.attention.value.weight\n",
      "encoder.layer.1.attention.value.bias\n",
      "encoder.layer.1.attention.output.weight\n",
      "encoder.layer.1.attention.output.bias\n",
      "encoder.layer.1.ln_1.weight\n",
      "encoder.layer.1.ln_1.bias\n",
      "encoder.layer.1.mlp.intermediate.weight\n",
      "encoder.layer.1.mlp.intermediate.bias\n",
      "encoder.layer.1.mlp.output.weight\n",
      "encoder.layer.1.mlp.output.bias\n",
      "encoder.layer.1.ln_2.weight\n",
      "encoder.layer.1.ln_2.bias\n",
      "encoder.layer.2.attention.query.weight\n",
      "encoder.layer.2.attention.query.bias\n",
      "encoder.layer.2.attention.key.weight\n",
      "encoder.layer.2.attention.key.bias\n",
      "encoder.layer.2.attention.value.weight\n",
      "encoder.layer.2.attention.value.bias\n",
      "encoder.layer.2.attention.output.weight\n",
      "encoder.layer.2.attention.output.bias\n",
      "encoder.layer.2.ln_1.weight\n",
      "encoder.layer.2.ln_1.bias\n",
      "encoder.layer.2.mlp.intermediate.weight\n",
      "encoder.layer.2.mlp.intermediate.bias\n",
      "encoder.layer.2.mlp.output.weight\n",
      "encoder.layer.2.mlp.output.bias\n",
      "encoder.layer.2.ln_2.weight\n",
      "encoder.layer.2.ln_2.bias\n",
      "encoder.layer.3.attention.query.weight\n",
      "encoder.layer.3.attention.query.bias\n",
      "encoder.layer.3.attention.key.weight\n",
      "encoder.layer.3.attention.key.bias\n",
      "encoder.layer.3.attention.value.weight\n",
      "encoder.layer.3.attention.value.bias\n",
      "encoder.layer.3.attention.output.weight\n",
      "encoder.layer.3.attention.output.bias\n",
      "encoder.layer.3.ln_1.weight\n",
      "encoder.layer.3.ln_1.bias\n",
      "encoder.layer.3.mlp.intermediate.weight\n",
      "encoder.layer.3.mlp.intermediate.bias\n",
      "encoder.layer.3.mlp.output.weight\n",
      "encoder.layer.3.mlp.output.bias\n",
      "encoder.layer.3.ln_2.weight\n",
      "encoder.layer.3.ln_2.bias\n",
      "encoder.layer.4.attention.query.weight\n",
      "encoder.layer.4.attention.query.bias\n",
      "encoder.layer.4.attention.key.weight\n",
      "encoder.layer.4.attention.key.bias\n",
      "encoder.layer.4.attention.value.weight\n",
      "encoder.layer.4.attention.value.bias\n",
      "encoder.layer.4.attention.output.weight\n",
      "encoder.layer.4.attention.output.bias\n",
      "encoder.layer.4.ln_1.weight\n",
      "encoder.layer.4.ln_1.bias\n",
      "encoder.layer.4.mlp.intermediate.weight\n",
      "encoder.layer.4.mlp.intermediate.bias\n",
      "encoder.layer.4.mlp.output.weight\n",
      "encoder.layer.4.mlp.output.bias\n",
      "encoder.layer.4.ln_2.weight\n",
      "encoder.layer.4.ln_2.bias\n",
      "encoder.layer.5.attention.query.weight\n",
      "encoder.layer.5.attention.query.bias\n",
      "encoder.layer.5.attention.key.weight\n",
      "encoder.layer.5.attention.key.bias\n",
      "encoder.layer.5.attention.value.weight\n",
      "encoder.layer.5.attention.value.bias\n",
      "encoder.layer.5.attention.output.weight\n",
      "encoder.layer.5.attention.output.bias\n",
      "encoder.layer.5.ln_1.weight\n",
      "encoder.layer.5.ln_1.bias\n",
      "encoder.layer.5.mlp.intermediate.weight\n",
      "encoder.layer.5.mlp.intermediate.bias\n",
      "encoder.layer.5.mlp.output.weight\n",
      "encoder.layer.5.mlp.output.bias\n",
      "encoder.layer.5.ln_2.weight\n",
      "encoder.layer.5.ln_2.bias\n",
      "encoder.layer.6.attention.query.weight\n",
      "encoder.layer.6.attention.query.bias\n",
      "encoder.layer.6.attention.key.weight\n",
      "encoder.layer.6.attention.key.bias\n",
      "encoder.layer.6.attention.value.weight\n",
      "encoder.layer.6.attention.value.bias\n",
      "encoder.layer.6.attention.output.weight\n",
      "encoder.layer.6.attention.output.bias\n",
      "encoder.layer.6.ln_1.weight\n",
      "encoder.layer.6.ln_1.bias\n",
      "encoder.layer.6.mlp.intermediate.weight\n",
      "encoder.layer.6.mlp.intermediate.bias\n",
      "encoder.layer.6.mlp.output.weight\n",
      "encoder.layer.6.mlp.output.bias\n",
      "encoder.layer.6.ln_2.weight\n",
      "encoder.layer.6.ln_2.bias\n",
      "encoder.layer.7.attention.query.weight\n",
      "encoder.layer.7.attention.query.bias\n",
      "encoder.layer.7.attention.key.weight\n",
      "encoder.layer.7.attention.key.bias\n",
      "encoder.layer.7.attention.value.weight\n",
      "encoder.layer.7.attention.value.bias\n",
      "encoder.layer.7.attention.output.weight\n",
      "encoder.layer.7.attention.output.bias\n",
      "encoder.layer.7.ln_1.weight\n",
      "encoder.layer.7.ln_1.bias\n",
      "encoder.layer.7.mlp.intermediate.weight\n",
      "encoder.layer.7.mlp.intermediate.bias\n",
      "encoder.layer.7.mlp.output.weight\n",
      "encoder.layer.7.mlp.output.bias\n",
      "encoder.layer.7.ln_2.weight\n",
      "encoder.layer.7.ln_2.bias\n",
      "encoder.layer.8.attention.query.weight\n",
      "encoder.layer.8.attention.query.bias\n",
      "encoder.layer.8.attention.key.weight\n",
      "encoder.layer.8.attention.key.bias\n",
      "encoder.layer.8.attention.value.weight\n",
      "encoder.layer.8.attention.value.bias\n",
      "encoder.layer.8.attention.output.weight\n",
      "encoder.layer.8.attention.output.bias\n",
      "encoder.layer.8.ln_1.weight\n",
      "encoder.layer.8.ln_1.bias\n",
      "encoder.layer.8.mlp.intermediate.weight\n",
      "encoder.layer.8.mlp.intermediate.bias\n",
      "encoder.layer.8.mlp.output.weight\n",
      "encoder.layer.8.mlp.output.bias\n",
      "encoder.layer.8.ln_2.weight\n",
      "encoder.layer.8.ln_2.bias\n",
      "encoder.layer.9.attention.query.weight\n",
      "encoder.layer.9.attention.query.bias\n",
      "encoder.layer.9.attention.key.weight\n",
      "encoder.layer.9.attention.key.bias\n",
      "encoder.layer.9.attention.value.weight\n",
      "encoder.layer.9.attention.value.bias\n",
      "encoder.layer.9.attention.output.weight\n",
      "encoder.layer.9.attention.output.bias\n",
      "encoder.layer.9.ln_1.weight\n",
      "encoder.layer.9.ln_1.bias\n",
      "encoder.layer.9.mlp.intermediate.weight\n",
      "encoder.layer.9.mlp.intermediate.bias\n",
      "encoder.layer.9.mlp.output.weight\n",
      "encoder.layer.9.mlp.output.bias\n",
      "encoder.layer.9.ln_2.weight\n",
      "encoder.layer.9.ln_2.bias\n",
      "encoder.layer.10.attention.query.weight\n",
      "encoder.layer.10.attention.query.bias\n",
      "encoder.layer.10.attention.key.weight\n",
      "encoder.layer.10.attention.key.bias\n",
      "encoder.layer.10.attention.value.weight\n",
      "encoder.layer.10.attention.value.bias\n",
      "encoder.layer.10.attention.output.weight\n",
      "encoder.layer.10.attention.output.bias\n",
      "encoder.layer.10.ln_1.weight\n",
      "encoder.layer.10.ln_1.bias\n",
      "encoder.layer.10.mlp.intermediate.weight\n",
      "encoder.layer.10.mlp.intermediate.bias\n",
      "encoder.layer.10.mlp.output.weight\n",
      "encoder.layer.10.mlp.output.bias\n",
      "encoder.layer.10.ln_2.weight\n",
      "encoder.layer.10.ln_2.bias\n",
      "encoder.layer.11.attention.query.weight\n",
      "encoder.layer.11.attention.query.bias\n",
      "encoder.layer.11.attention.key.weight\n",
      "encoder.layer.11.attention.key.bias\n",
      "encoder.layer.11.attention.value.weight\n",
      "encoder.layer.11.attention.value.bias\n",
      "encoder.layer.11.attention.output.weight\n",
      "encoder.layer.11.attention.output.bias\n",
      "encoder.layer.11.ln_1.weight\n",
      "encoder.layer.11.ln_1.bias\n",
      "encoder.layer.11.mlp.intermediate.weight\n",
      "encoder.layer.11.mlp.intermediate.bias\n",
      "encoder.layer.11.mlp.output.weight\n",
      "encoder.layer.11.mlp.output.bias\n",
      "encoder.layer.11.ln_2.weight\n",
      "encoder.layer.11.ln_2.bias\n",
      "pooler.weight\n",
      "pooler.bias\n"
     ]
    }
   ],
   "source": [
    "for key in model.state_dict().keys():\n",
    "    print(key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
