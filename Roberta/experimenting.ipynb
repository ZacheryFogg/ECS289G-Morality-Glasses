{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zfogg\\anaconda3\\envs\\torch-cuda12.4\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "from typing import List\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from model import RobertaClassificationAndLM\n",
    "from data import EthicsDataset, MoralStoriesDataset, WikiTextDataset, morality_classification_examples, morality_probing_examples_hard\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pynvml import *\n",
    "from matplotlib.pyplot import figure\n",
    "import time\n",
    "\n",
    "from helper import create_attention_mask, calculate_accuracy_loss, train_model, print_gpu_utilization, get_gpu_mem_usage, calculate_wikitext_loss\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    \n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "@dataclass \n",
    "class RobertaBaseConfig:\n",
    "    mod_layers: list = field(default_factory=lambda: list(range(12)))\n",
    "    vocab_size: int = 50265\n",
    "    hidden_size: int = 768\n",
    "    num_hidden_layers: int = 12\n",
    "    num_attention_heads: int = 12\n",
    "    intermediate_size: int = 3072\n",
    "    max_position_embeddings: int = 514\n",
    "    layer_norm_eps: float = 1e-05\n",
    "    num_class_labels: int = 1\n",
    "    pad_token_id: int = 1\n",
    "\n",
    "    # Special Configs \n",
    "    rank: int = None\n",
    "    attn_type: str = 'spda'\n",
    "    use_bottleneck: bool = False\n",
    "    bottleneck_size: int = None\n",
    "    prefix_size: int = None\n",
    "    use_prefix: bool = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49.267169179229484, 49.0787269681742, 0.08937899768352509, 1.5672330856323242)\n"
     ]
    }
   ],
   "source": [
    "d = torch.load('./Datasets/ethics/test_dataset_ethics.pt', weights_only= False)\n",
    "d = DataLoader(d, batch_size=32)\n",
    "m = RobertaClassificationAndLM.from_pretrained(RobertaBaseConfig(use_bottleneck = True, bottleneck_size = 8, mod_layers= list(range(6,12))), size = 'base')\n",
    "m.load_state_dict(torch.load('./trained_models/base/adapter_900', weights_only= True))\n",
    "\n",
    "m = m.to(device)\n",
    "\n",
    "print(calculate_accuracy_loss(m, d, device, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "moral_pref = \"This is moral: \"\n",
    "immoral_pref = \"This is immmoral: \"\n",
    "neutral_pref  = \"This is neutral: \"\n",
    "\n",
    "def get_probs(model, x, device, prefix_size):\n",
    "    x = tokenizer.encode(x)\n",
    "    x = torch.tensor(x).unsqueeze(0).to(device)\n",
    "    \n",
    "    attn_mask = create_attention_mask(x, device, dtype = torch.bfloat16, prefix_size= prefix_size)\n",
    "    attn_mask = attn_mask.to(torch.float32)\n",
    "\n",
    "    with torch.no_grad() and torch.autocast(device_type = device, dtype = torch.bfloat16):\n",
    "        logits, _, _ = model(x, attention_mask = attn_mask, run_lm_head = True)\n",
    "\n",
    "    mask_token_index = (x == tokenizer.mask_token_id)[0].nonzero(as_tuple = True)[0]\n",
    "\n",
    "    probs = F.softmax(logits[0, mask_token_index], dim=1)\n",
    "\n",
    "    return probs \n",
    "\n",
    "def collect_ratios(model, data, device, tokenizer, prefix_size = 0):\n",
    "    seq = data[\"Seq\"]\n",
    "    moral_token = tokenizer.encode(data[\"Moral\"])[1:-1]\n",
    "    immoral_token = tokenizer.encode(data[\"Immoral\"])[1:-1]\n",
    "\n",
    "    if len(moral_token) != 1 or len(immoral_token) != 1: \n",
    "        print(f'Could not encoder targets to single token: {data}' )\n",
    "        return None, None, None\n",
    "    \n",
    "    moral_token = moral_token[0]\n",
    "    immoral_token = immoral_token[0]\n",
    "\n",
    "    moral_probs = get_probs(model, moral_pref + seq, device, prefix_size= prefix_size).view(-1)\n",
    "    immoral_probs = get_probs(model, immoral_pref + seq, device, prefix_size= prefix_size).view(-1)\n",
    "    neutral_probs = get_probs(model, neutral_pref + seq, device, prefix_size= prefix_size).view(-1)\n",
    "\n",
    "    # How much more likely model thinks that moral token is compared to the immoral token\n",
    "\n",
    "    # Ratio for model that was hopefully conditioned to choose a produce moral text\n",
    "    moral_ratio = (moral_probs[moral_token] / moral_probs[immoral_token]).item() \n",
    "    \n",
    "    # Ratio for model that was hopefully conditioned to choose a produce immoral text\n",
    "    immoral_ratio = (immoral_probs[moral_token] / immoral_probs[immoral_token]).item()\n",
    "\n",
    "    # Ratio for model that was not conditioned to produce moral or immoral text\n",
    "    neutral_ratio = (neutral_probs[moral_token] / neutral_probs[immoral_token]).item()\n",
    "\n",
    "    return moral_ratio, neutral_ratio, immoral_ratio\n",
    "\n",
    "def get_top_k_preds(model, x, device, prefix_size = 0, k = 5):\n",
    "    probs = get_probs(model, x, device, prefix_size)\n",
    "    \n",
    "    topk = torch.topk(probs, k)\n",
    "\n",
    "    topk = [(tokenizer.decode(topk.indices.squeeze()[i].item()), round(topk.values.squeeze()[i].item() * 100, 2)) for i in range(topk.indices.shape[1])]\n",
    "    \n",
    "    return topk\n",
    "\n",
    "\n",
    "def moral_prediction_accuracy(model, dataset, device, tokenizer, prefix_size = 0):\n",
    "\n",
    "    correct = 0\n",
    "\n",
    "    for data in dataset:\n",
    "        moral_ratio, neutral_ratio, immoral_ratio = collect_ratios(model, data, device, tokenizer, prefix_size)\n",
    "\n",
    "        '''\n",
    "        If model has learned to output moral/immmoral text, then it moral_ratio should be higher then neutral since the model should     \n",
    "        prefer the moral word more than it did before and prefer the immoral word less than is did before \n",
    "\n",
    "        Immoral_ratio should be less than neutral ratio since the model should prefer the moral word less than it did before and \n",
    "        prefer the immoral word more than it did before\n",
    "\n",
    "        This intuition is expressed in this equality \n",
    "        '''\n",
    "\n",
    "        if moral_ratio > neutral_ratio and neutral_ratio > immoral_ratio:\n",
    "            correct +=1 \n",
    "\n",
    "    return round((correct  / len(dataset)) * 100, 2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " kill       20.12\n",
      " scare      7.37\n",
      " harass     3.96\n",
      " hurt       3.96\n",
      " attack     3.96\n",
      "\n",
      "\n",
      " kill       20.8\n",
      " silence    5.96\n",
      " scare      4.64\n",
      " hurt       4.64\n",
      " treat      4.1\n"
     ]
    }
   ],
   "source": [
    "x = \"The man approached the neighbors dog and decided to<mask> it.\"\n",
    "\n",
    "topk = get_top_k_preds(m, 'This is immoral: ' + x, device)\n",
    "\n",
    "for pred in topk:\n",
    "    print(f'{pred[0] : <10}  {pred[1]}')\n",
    "\n",
    "print('\\n')\n",
    "topk = get_top_k_preds(m, 'This is a moral action: ' + x, device)\n",
    "\n",
    "for pred in topk:\n",
    "    print(f'{pred[0] : <10}  {pred[1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cuda12.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
